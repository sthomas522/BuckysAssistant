{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ElDDZDDNF4sZ"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install --upgrade langchain_openai langchain_core langgraph SPARQLWrapper\n",
        "!pip install --upgrade duckduckgo-search wikipedia wikipedia-api\n",
        "!pip install --upgrade opencv-python yt-dlp pytube\n",
        "!pip install --upgrade langchain_huggingface langchain_community datasets gradio\n",
        "!pip install --upgrade pillow spacy librosa\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain faiss-cpu\n",
        "!pip install pyppeteer\n",
        "!pip install ipdb\n",
        "!pip install beautifulsoup4\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC9HTSHJF9aC",
        "outputId": "e50924d4-3958-4ddb-e293-4f7dcdcfbc3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.2->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.0\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.63)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.82.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (0.3.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.5)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain_core) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain_core) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain_core) (2.4.0)\n",
            "Downloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib, ormsgpack, SPARQLWrapper, langgraph-sdk, langgraph-checkpoint, langchain_openai, langgraph-prebuilt, langgraph\n",
            "Successfully installed SPARQLWrapper-2.0.0 langchain_openai-0.3.19 langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0 rdflib-7.1.4\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.0.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
            "Downloading duckduckgo_search-8.0.2-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia, wikipedia-api\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=7e90a4bb941518ea74089c9b100c333fef2ce6ff207c34002209d741ec96c63c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=454c0f2cb7952df4e0185a33d292441701e70e4cf60fa4a010a79e7e149c4323\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "Successfully built wikipedia wikipedia-api\n",
            "Installing collected packages: primp, wikipedia-api, wikipedia, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.0.2 primp-0.15.0 wikipedia-1.4.0 wikipedia-api-0.8.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.5.22-py3-none-any.whl.metadata (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading yt_dlp-2025.5.22-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp, pytube\n",
            "Successfully installed pytube-15.0.0 yt-dlp-2025.5.22\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.63)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.52.3)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.32.2)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.43)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.15.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, fsspec, typing-inspect, pydantic-settings, gradio-client, dataclasses-json, gradio, datasets, langchain_huggingface, langchain_community\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.10.1\n",
            "    Uninstalling gradio_client-1.10.1:\n",
            "      Successfully uninstalled gradio_client-1.10.1\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.31.0\n",
            "    Uninstalling gradio-5.31.0:\n",
            "      Successfully uninstalled gradio-5.31.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 datasets-3.6.0 fsspec-2025.3.0 gradio-5.32.1 gradio-client-1.10.2 httpx-sse-0.4.0 langchain_community-0.3.24 langchain_huggingface-0.2.0 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer) (2025.4.26)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.22.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.13.2)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, websockets, urllib3, pyee, pyppeteer\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.17.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.7.5 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.61 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 pyee-11.1.1 pyppeteer-2.0.0 urllib3-1.26.20 websockets-10.4\n",
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.2\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting urllib3~=2.4.0 (from urllib3[socks]~=2.4.0->selenium)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, urllib3, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.20\n",
            "    Uninstalling urllib3-1.26.20:\n",
            "      Successfully uninstalled urllib3-1.26.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyppeteer 2.0.0 requires urllib3<2.0.0,>=1.25.8, but you have urllib3 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 urllib3-2.4.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "potential_urls = [\n",
        "    'https://localauctions.com/',\n",
        "    'https://www.auctionzip.com/Auctioneer-Directory/',\n",
        "    'https://www.liveauctioneers.com/catalog/search/',\n",
        "    'https://localauctionsnetwork.com/',\n",
        "    'https://www.proxibid.com/auctions-near-me-map',\n",
        "    'https://www.3bsauction.com/',\n",
        "    'https://www.aetherestateservices.com/',\n",
        "    'https://flipsideestates.com/',\n",
        "    'https://hibid.com/indiana',\n",
        "    'https://unitedcountrycoffey.hibid.com/',\n",
        "    'https://www.earlsauction.com/',\n",
        "    'https://bidwickliff.com/',\n",
        "    'https://www.christys.com/',\n",
        "]"
      ],
      "metadata": {
        "id": "VpDVDgU9GFmr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "che_QyaeRYSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hibid scraper"
      ],
      "metadata": {
        "id": "f46AJybLRZXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import sqlite3\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, List, Dict\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class AuctionItem:\n",
        "    lot_number: str\n",
        "    description: str\n",
        "    current_price: Optional[float]\n",
        "    price_text: str\n",
        "    bid_count: int\n",
        "    source: str\n",
        "    auction_id: Optional[str] = None\n",
        "    end_time: Optional[str] = None\n",
        "    time_remaining: Optional[str] = None\n",
        "    image_urls: List[str] = None\n",
        "    auction_title: Optional[str] = None\n",
        "    company_name: Optional[str] = None\n",
        "    scraped_at: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.image_urls is None:\n",
        "            self.image_urls = []\n",
        "        if self.scraped_at is None:\n",
        "            self.scraped_at = datetime.now().isoformat()\n",
        "\n",
        "@dataclass\n",
        "class AuctionInfo:\n",
        "    company_name: str\n",
        "    company_url: str\n",
        "    auction_title: str\n",
        "    dates: str\n",
        "    location: str\n",
        "    bidding_notice: str\n",
        "    zip_code: Optional[str]\n",
        "    end_time: Optional[str] = None\n",
        "    time_remaining: Optional[str] = None\n",
        "    auction_id: Optional[str] = None\n",
        "    scraped_at: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.scraped_at is None:\n",
        "            self.scraped_at = datetime.now().isoformat()\n",
        "\n",
        "class DatabaseManager:\n",
        "    def __init__(self, db_path='hibid_auctions.db'):\n",
        "        self.db_path = db_path\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Initialize the database with required tables\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            # Auctions table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS auctions (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    auction_id TEXT UNIQUE,\n",
        "                    company_name TEXT,\n",
        "                    company_url TEXT,\n",
        "                    auction_title TEXT,\n",
        "                    dates TEXT,\n",
        "                    location TEXT,\n",
        "                    bidding_notice TEXT,\n",
        "                    zip_code TEXT,\n",
        "                    end_time TEXT,\n",
        "                    time_remaining TEXT,\n",
        "                    scraped_at TEXT,\n",
        "                    UNIQUE(auction_id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Items table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS items (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    lot_number TEXT,\n",
        "                    description TEXT,\n",
        "                    current_price REAL,\n",
        "                    price_text TEXT,\n",
        "                    bid_count INTEGER,\n",
        "                    source TEXT,\n",
        "                    auction_id TEXT,\n",
        "                    end_time TEXT,\n",
        "                    time_remaining TEXT,\n",
        "                    auction_title TEXT,\n",
        "                    company_name TEXT,\n",
        "                    scraped_at TEXT,\n",
        "                    FOREIGN KEY (auction_id) REFERENCES auctions (auction_id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Images table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS item_images (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    item_id INTEGER,\n",
        "                    image_url TEXT,\n",
        "                    image_order INTEGER,\n",
        "                    FOREIGN KEY (item_id) REFERENCES items (id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            conn.commit()\n",
        "\n",
        "    def save_auction(self, auction: AuctionInfo) -> int:\n",
        "        \"\"\"Save auction info to database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                INSERT OR REPLACE INTO auctions\n",
        "                (auction_id, company_name, company_url, auction_title, dates,\n",
        "                 location, bidding_notice, zip_code, end_time, time_remaining, scraped_at)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                auction.auction_id, auction.company_name, auction.company_url,\n",
        "                auction.auction_title, auction.dates, auction.location,\n",
        "                auction.bidding_notice, auction.zip_code, auction.end_time,\n",
        "                auction.time_remaining, auction.scraped_at\n",
        "            ))\n",
        "            return cursor.lastrowid\n",
        "\n",
        "    def save_item(self, item: AuctionItem) -> int:\n",
        "        \"\"\"Save auction item to database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                INSERT INTO items\n",
        "                (lot_number, description, current_price, price_text, bid_count,\n",
        "                 source, auction_id, end_time, time_remaining, auction_title,\n",
        "                 company_name, scraped_at)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                item.lot_number, item.description, item.current_price,\n",
        "                item.price_text, item.bid_count, item.source, item.auction_id,\n",
        "                item.end_time, item.time_remaining, item.auction_title,\n",
        "                item.company_name, item.scraped_at\n",
        "            ))\n",
        "\n",
        "            item_id = cursor.lastrowid\n",
        "\n",
        "            # Save images\n",
        "            for i, image_url in enumerate(item.image_urls):\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO item_images (item_id, image_url, image_order)\n",
        "                    VALUES (?, ?, ?)\n",
        "                ''', (item_id, image_url, i))\n",
        "\n",
        "            return item_id\n",
        "\n",
        "    def get_active_auctions(self, zip_code: Optional[str] = None) -> List[Dict]:\n",
        "        \"\"\"Get active auctions from database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            query = '''\n",
        "                SELECT * FROM auctions\n",
        "                WHERE datetime(scraped_at) > datetime('now', '-24 hours')\n",
        "            '''\n",
        "            params = []\n",
        "\n",
        "            if zip_code:\n",
        "                query += ' AND zip_code = ?'\n",
        "                params.append(zip_code)\n",
        "\n",
        "            query += ' ORDER BY scraped_at DESC'\n",
        "\n",
        "            cursor.execute(query, params)\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "\n",
        "    def get_items_by_auction(self, auction_id: str) -> List[Dict]:\n",
        "        \"\"\"Get all items for a specific auction\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                SELECT i.*, GROUP_CONCAT(img.image_url) as image_urls\n",
        "                FROM items i\n",
        "                LEFT JOIN item_images img ON i.id = img.item_id\n",
        "                WHERE i.auction_id = ?\n",
        "                GROUP BY i.id\n",
        "                ORDER BY CAST(i.lot_number AS INTEGER)\n",
        "            ''', (auction_id,))\n",
        "\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            items = []\n",
        "            for row in cursor.fetchall():\n",
        "                item = dict(zip(columns, row))\n",
        "                if item['image_urls']:\n",
        "                    item['image_urls'] = item['image_urls'].split(',')\n",
        "                else:\n",
        "                    item['image_urls'] = []\n",
        "                items.append(item)\n",
        "            return items\n",
        "\n",
        "class EnhancedHiBidScraper:\n",
        "    def __init__(self, zip_code=None, db_path='hibid_auctions.db'):\n",
        "        self.base_url = \"https://hibid.com\"\n",
        "        self.indiana_url = \"https://hibid.com/indiana\"\n",
        "        self.zip_code = zip_code\n",
        "        if zip_code:\n",
        "            self.zip_url = f\"https://hibid.com/indiana/auctions?zip={zip_code}\"\n",
        "        else:\n",
        "            self.zip_url = None\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "        self.db = DatabaseManager(db_path)\n",
        "\n",
        "    def get_page_content(self, url, retries=3):\n",
        "        \"\"\"Fetch page content with retry logic\"\"\"\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                return response.text\n",
        "            except requests.RequestException as e:\n",
        "                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def parse_price(self, price_text):\n",
        "        \"\"\"Extract numeric price from price text\"\"\"\n",
        "        if not price_text:\n",
        "            return None\n",
        "\n",
        "        # Remove common currency symbols and spaces\n",
        "        cleaned = re.sub(r'[^\\d.,]', '', price_text)\n",
        "        price_match = re.search(r'\\d+(?:,\\d{3})*(?:\\.\\d{2})?', cleaned)\n",
        "        if price_match:\n",
        "            try:\n",
        "                return float(price_match.group().replace(',', ''))\n",
        "            except ValueError:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def parse_time_remaining(self, time_text):\n",
        "        \"\"\"Parse time remaining from various formats\"\"\"\n",
        "        if not time_text:\n",
        "            return None, None\n",
        "\n",
        "        # Clean up the text\n",
        "        time_text = re.sub(r'\\s+', ' ', time_text.strip())\n",
        "\n",
        "        # Common patterns for time remaining\n",
        "        patterns = [\n",
        "            r'(\\d+)d\\s*(\\d+)h\\s*(\\d+)m',  # 5d 12h 30m\n",
        "            r'(\\d+)\\s*days?\\s*(\\d+)\\s*hours?\\s*(\\d+)\\s*min',\n",
        "            r'(\\d+)h\\s*(\\d+)m',  # 12h 30m\n",
        "            r'(\\d+)\\s*hours?\\s*(\\d+)\\s*min',\n",
        "            r'(\\d+)m',  # 30m\n",
        "            r'(\\d+)\\s*min',\n",
        "            r'Ends?:?\\s*(.+)',  # Ends: Dec 15, 2024 3:00 PM\n",
        "            r'Closing:?\\s*(.+)',  # Closing: Dec 15, 2024\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, time_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                groups = match.groups()\n",
        "\n",
        "                if len(groups) == 3:  # days, hours, minutes\n",
        "                    try:\n",
        "                        days, hours, minutes = map(int, groups)\n",
        "                        end_time = datetime.now() + timedelta(days=days, hours=hours, minutes=minutes)\n",
        "                        return end_time.isoformat(), f\"{days}d {hours}h {minutes}m\"\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                elif len(groups) == 2 and any(x in time_text.lower() for x in ['h', 'hour']):  # hours, minutes\n",
        "                    try:\n",
        "                        hours, minutes = map(int, groups)\n",
        "                        end_time = datetime.now() + timedelta(hours=hours, minutes=minutes)\n",
        "                        return end_time.isoformat(), f\"{hours}h {minutes}m\"\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                elif len(groups) == 1:\n",
        "                    if any(x in time_text.lower() for x in ['m', 'min']):  # minutes only\n",
        "                        try:\n",
        "                            minutes = int(groups[0])\n",
        "                            end_time = datetime.now() + timedelta(minutes=minutes)\n",
        "                            return end_time.isoformat(), f\"{minutes}m\"\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "                    else:  # Absolute time\n",
        "                        try:\n",
        "                            # Try various date formats\n",
        "                            date_formats = [\n",
        "                                \"%b %d, %Y %I:%M %p\",\n",
        "                                \"%B %d, %Y %I:%M %p\",\n",
        "                                \"%m/%d/%Y %I:%M %p\",\n",
        "                                \"%m-%d-%Y %I:%M %p\",\n",
        "                                \"%Y-%m-%d %H:%M:%S\",\n",
        "                                \"%m/%d/%Y %H:%M\",\n",
        "                            ]\n",
        "\n",
        "                            date_str = groups[0].strip()\n",
        "                            for fmt in date_formats:\n",
        "                                try:\n",
        "                                    end_time = datetime.strptime(date_str, fmt)\n",
        "                                    remaining = end_time - datetime.now()\n",
        "                                    if remaining.total_seconds() > 0:\n",
        "                                        days = remaining.days\n",
        "                                        hours, remainder = divmod(remaining.seconds, 3600)\n",
        "                                        minutes, _ = divmod(remainder, 60)\n",
        "                                        return end_time.isoformat(), f\"{days}d {hours}h {minutes}m\"\n",
        "                                    break\n",
        "                                except ValueError:\n",
        "                                    continue\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "        return None, time_text\n",
        "\n",
        "    def is_valid_image_url(self, url):\n",
        "        \"\"\"Check if URL is a valid image URL\"\"\"\n",
        "        if not url:\n",
        "            return False\n",
        "\n",
        "        # Check for common image extensions\n",
        "        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        # Check extension\n",
        "        if any(ext in url_lower for ext in image_extensions):\n",
        "            return True\n",
        "\n",
        "        # Check for image-related keywords in URL\n",
        "        image_keywords = ['image', 'img', 'photo', 'picture', 'thumb', 'gallery']\n",
        "        if any(keyword in url_lower for keyword in image_keywords):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def extract_images(self, soup, base_url):\n",
        "        \"\"\"Enhanced image extraction with multiple strategies\"\"\"\n",
        "        images = []\n",
        "\n",
        "        # Strategy 1: Look for all img tags and filter by various criteria\n",
        "        all_imgs = soup.find_all('img')\n",
        "\n",
        "        for img in all_imgs:\n",
        "            # Get src or data-src\n",
        "            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')\n",
        "\n",
        "            if not src:\n",
        "                continue\n",
        "\n",
        "            # Convert to absolute URL\n",
        "            full_url = urljoin(base_url, src)\n",
        "\n",
        "            # Skip common non-item images\n",
        "            skip_patterns = [\n",
        "                'logo', 'banner', 'header', 'footer', 'icon',\n",
        "                'avatar', 'profile', 'social', 'ad', 'advertisement',\n",
        "                'placeholder', 'loading', 'spinner', '1x1', 'tracking'\n",
        "            ]\n",
        "\n",
        "            if any(pattern in full_url.lower() for pattern in skip_patterns):\n",
        "                continue\n",
        "\n",
        "            # Check if it's a valid image URL\n",
        "            if self.is_valid_image_url(full_url):\n",
        "                # Check image dimensions (if available) to skip tiny images\n",
        "                width = img.get('width')\n",
        "                height = img.get('height')\n",
        "\n",
        "                if width and height:\n",
        "                    try:\n",
        "                        w, h = int(width), int(height)\n",
        "                        if w < 50 or h < 50:  # Skip very small images\n",
        "                            continue\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "                # Check CSS classes for lot/item related images\n",
        "                img_classes = img.get('class', [])\n",
        "                if isinstance(img_classes, str):\n",
        "                    img_classes = img_classes.split()\n",
        "\n",
        "                # Prioritize images with lot/item related classes\n",
        "                priority_classes = ['lot', 'item', 'product', 'auction', 'photo', 'image', 'gallery']\n",
        "                has_priority = any(cls.lower() in ' '.join(img_classes).lower() for cls in priority_classes)\n",
        "\n",
        "                if full_url not in images:\n",
        "                    if has_priority:\n",
        "                        images.insert(0, full_url)  # Add priority images first\n",
        "                    else:\n",
        "                        images.append(full_url)\n",
        "\n",
        "        # Strategy 2: Look for background images in CSS\n",
        "        for element in soup.find_all(attrs={\"style\": True}):\n",
        "            style = element.get('style', '')\n",
        "            bg_match = re.search(r'background-image:\\s*url\\([\"\\']?([^\"\\']+)[\"\\']?\\)', style)\n",
        "            if bg_match:\n",
        "                bg_url = urljoin(base_url, bg_match.group(1))\n",
        "                if self.is_valid_image_url(bg_url) and bg_url not in images:\n",
        "                    images.append(bg_url)\n",
        "\n",
        "        # Strategy 3: Look for data attributes that might contain image URLs\n",
        "        for element in soup.find_all():\n",
        "            if hasattr(element, 'attrs') and element.attrs:\n",
        "                for attr, value in element.attrs.items():\n",
        "                    if (isinstance(attr, str) and attr.startswith('data-') and\n",
        "                        'img' in attr.lower() and isinstance(value, str)):\n",
        "                        if 'http' in value or value.startswith('/'):\n",
        "                            img_url = urljoin(base_url, value)\n",
        "                            if self.is_valid_image_url(img_url) and img_url not in images:\n",
        "                                images.append(img_url)\n",
        "\n",
        "        # Strategy 4: Look for JSON-LD or other structured data\n",
        "        json_scripts = soup.find_all('script', type='application/ld+json')\n",
        "        for script in json_scripts:\n",
        "            try:\n",
        "                data = json.loads(script.string)\n",
        "                if isinstance(data, dict):\n",
        "                    # Look for image fields\n",
        "                    for key, value in data.items():\n",
        "                        if 'image' in key.lower() and isinstance(value, str):\n",
        "                            img_url = urljoin(base_url, value)\n",
        "                            if self.is_valid_image_url(img_url) and img_url not in images:\n",
        "                                images.append(img_url)\n",
        "            except (json.JSONDecodeError, TypeError):\n",
        "                continue\n",
        "\n",
        "        # Clean and deduplicate\n",
        "        cleaned_images = []\n",
        "        for img_url in images[:10]:  # Limit to 10 images\n",
        "            # Clean up URL\n",
        "            parsed = urlparse(img_url)\n",
        "            if parsed.scheme and parsed.netloc:\n",
        "                cleaned_images.append(img_url)\n",
        "\n",
        "        logger.info(f\"Found {len(cleaned_images)} images for lot\")\n",
        "        return cleaned_images[:5]  # Return max 5 images\n",
        "\n",
        "    def extract_auction_id(self, url):\n",
        "        \"\"\"Extract auction ID from URL\"\"\"\n",
        "        patterns = [\n",
        "            r'/auction/(\\d+)',\n",
        "            r'/catalog/(\\d+)',\n",
        "            r'/company/(\\d+)',\n",
        "            r'auction_id=(\\d+)',\n",
        "            r'/auctions/(\\d+)',\n",
        "            r'/sale/(\\d+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, url)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def scrape_catalog_page_enhanced(self, catalog_url):\n",
        "        \"\"\"Enhanced catalog page scraping with improved image extraction\"\"\"\n",
        "        logger.info(f\"Scraping catalog: {catalog_url}\")\n",
        "        content = self.get_page_content(catalog_url)\n",
        "        if not content:\n",
        "            logger.warning(f\"Could not fetch content from {catalog_url}\")\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        items = []\n",
        "\n",
        "        # Extract auction info\n",
        "        auction_id = self.extract_auction_id(catalog_url)\n",
        "        auction_title = \"\"\n",
        "        company_name = \"\"\n",
        "\n",
        "        # Look for auction title in various places\n",
        "        title_selectors = [\n",
        "            'h1', 'h2.auction-title', '.title', '.auction-name',\n",
        "            '[class*=\"title\"]', '[class*=\"auction\"]', 'title'\n",
        "        ]\n",
        "\n",
        "        for selector in title_selectors:\n",
        "            title_elem = soup.select_one(selector)\n",
        "            if title_elem:\n",
        "                title_text = title_elem.get_text().strip()\n",
        "                if len(title_text) > 5 and 'hibid' not in title_text.lower():\n",
        "                    auction_title = title_text[:200]\n",
        "                    break\n",
        "\n",
        "        # Look for company name\n",
        "        company_selectors = [\n",
        "            '.company-name', '.auctioneer', '[class*=\"company\"]',\n",
        "            '[class*=\"auctioneer\"]', '.seller'\n",
        "        ]\n",
        "\n",
        "        for selector in company_selectors:\n",
        "            company_elem = soup.select_one(selector)\n",
        "            if company_elem:\n",
        "                company_text = company_elem.get_text().strip()\n",
        "                if len(company_text) > 2:\n",
        "                    company_name = company_text[:100]\n",
        "                    break\n",
        "\n",
        "        # Extract auction end time\n",
        "        auction_end_time = None\n",
        "        auction_time_remaining = None\n",
        "\n",
        "        # Look for time information\n",
        "        time_selectors = [\n",
        "            '[class*=\"time\"]', '[class*=\"end\"]', '[class*=\"closing\"]',\n",
        "            '.auction-time', '.end-time', '.closing-time'\n",
        "        ]\n",
        "\n",
        "        for selector in time_selectors:\n",
        "            time_elem = soup.select_one(selector)\n",
        "            if time_elem:\n",
        "                time_text = time_elem.get_text()\n",
        "                end_time, time_remaining = self.parse_time_remaining(time_text)\n",
        "                if end_time:\n",
        "                    auction_end_time = end_time\n",
        "                    auction_time_remaining = time_remaining\n",
        "                    break\n",
        "\n",
        "        # Look for lot containers with multiple strategies\n",
        "        lot_containers = []\n",
        "\n",
        "        # Strategy 1: Look for elements with lot/item in class or id\n",
        "        lot_selectors = [\n",
        "            '[class*=\"lot\"]', '[id*=\"lot\"]',\n",
        "            '[class*=\"item\"]', '[id*=\"item\"]',\n",
        "            'tr[class*=\"auction\"]', 'div[class*=\"auction\"]',\n",
        "            '.product', '.listing'\n",
        "        ]\n",
        "\n",
        "        for selector in lot_selectors:\n",
        "            elements = soup.select(selector)\n",
        "            if elements:\n",
        "                lot_containers.extend(elements)\n",
        "                logger.info(f\"Found {len(elements)} elements with selector: {selector}\")\n",
        "\n",
        "        # Strategy 2: Look for table rows that might contain lots\n",
        "        table_rows = soup.select('tr')\n",
        "        for row in table_rows:\n",
        "            row_text = row.get_text().lower()\n",
        "            if any(keyword in row_text for keyword in ['lot', 'item', 'bid', '$']):\n",
        "                lot_containers.append(row)\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        unique_containers = []\n",
        "        for container in lot_containers:\n",
        "            container_id = id(container)\n",
        "            if container_id not in seen:\n",
        "                seen.add(container_id)\n",
        "                unique_containers.append(container)\n",
        "\n",
        "        logger.info(f\"Processing {len(unique_containers)} potential lot containers\")\n",
        "\n",
        "        for i, lot_elem in enumerate(unique_containers):\n",
        "            try:\n",
        "                lot_text = lot_elem.get_text()\n",
        "\n",
        "                # Skip elements that are too short or don't look like lots\n",
        "                if len(lot_text.strip()) < 10:\n",
        "                    continue\n",
        "\n",
        "                # Extract lot number with more flexible patterns\n",
        "                lot_patterns = [\n",
        "                    r'(?:Lot|Item|#)\\s*[:\\-]?\\s*(\\d+[a-zA-Z]?)',\n",
        "                    r'(?:^|\\s)(\\d+[a-zA-Z]?)[:\\-]',  # Number at start of line or after space\n",
        "                    r'#(\\d+[a-zA-Z]?)',\n",
        "                    r'(\\d{1,4}[a-zA-Z]?)\\s*(?:\\.|:|\\-)',  # Number followed by punctuation\n",
        "                ]\n",
        "\n",
        "                lot_number = None\n",
        "                for pattern in lot_patterns:\n",
        "                    lot_match = re.search(pattern, lot_text, re.IGNORECASE)\n",
        "                    if lot_match:\n",
        "                        potential_lot = lot_match.group(1)\n",
        "                        # Validate lot number (should be reasonable)\n",
        "                        if potential_lot.isdigit() and 1 <= int(potential_lot) <= 9999:\n",
        "                            lot_number = potential_lot\n",
        "                            break\n",
        "                        elif len(potential_lot) <= 6:  # Allow alphanumeric lot numbers\n",
        "                            lot_number = potential_lot\n",
        "                            break\n",
        "\n",
        "                if not lot_number:\n",
        "                    continue\n",
        "\n",
        "                # Extract description with multiple strategies\n",
        "                description = \"No description\"\n",
        "\n",
        "                # Strategy 1: Look for specific description elements\n",
        "                desc_selectors = [\n",
        "                    '.description', '.title', '.name', '.item-title',\n",
        "                    'h1', 'h2', 'h3', 'h4', 'h5', 'strong', 'b'\n",
        "                ]\n",
        "\n",
        "                for selector in desc_selectors:\n",
        "                    desc_elem = lot_elem.select_one(selector)\n",
        "                    if desc_elem:\n",
        "                        desc_text = desc_elem.get_text().strip()\n",
        "                        if len(desc_text) > 5 and not re.match(r'^(Lot|#|\\d+)', desc_text):\n",
        "                            description = desc_text[:300]\n",
        "                            break\n",
        "\n",
        "                # Strategy 2: Extract from text content\n",
        "                if description == \"No description\":\n",
        "                    text_parts = [part.strip() for part in lot_text.split('\\n') if part.strip()]\n",
        "                    for part in text_parts:\n",
        "                        # Skip short parts, lot numbers, prices, and bid info\n",
        "                        if (len(part) > 15 and\n",
        "                            not re.match(r'^(Lot|#|\\d+)', part) and\n",
        "                            '$' not in part and\n",
        "                            'bid' not in part.lower()):\n",
        "                            description = part[:300]\n",
        "                            break\n",
        "\n",
        "                # Extract price information\n",
        "                current_price = None\n",
        "                price_text = \"\"\n",
        "\n",
        "                price_patterns = [\n",
        "                    r'\\$(\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)',\n",
        "                    r'USD\\s*(\\d+(?:\\.\\d{2})?)',\n",
        "                    r'Current[:\\s]*\\$?(\\d+(?:\\.\\d{2})?)',\n",
        "                    r'Bid[:\\s]*\\$?(\\d+(?:\\.\\d{2})?)',\n",
        "                    r'Price[:\\s]*\\$?(\\d+(?:\\.\\d{2})?)'\n",
        "                ]\n",
        "\n",
        "                for pattern in price_patterns:\n",
        "                    price_match = re.search(pattern, lot_text)\n",
        "                    if price_match:\n",
        "                        price_text = price_match.group(0)\n",
        "                        current_price = self.parse_price(price_text)\n",
        "                        if current_price and current_price > 0:\n",
        "                            break\n",
        "\n",
        "                # Extract bid count\n",
        "                bid_count = 0\n",
        "                bid_patterns = [\n",
        "                    r'(\\d+)\\s*Bids?',\n",
        "                    r'Bids?[:\\s]*(\\d+)',\n",
        "                    r'(\\d+)\\s*(?:bidders?|bids?)'\n",
        "                ]\n",
        "\n",
        "                for pattern in bid_patterns:\n",
        "                    bid_match = re.search(pattern, lot_text, re.IGNORECASE)\n",
        "                    if bid_match:\n",
        "                        try:\n",
        "                            bid_count = int(bid_match.group(1))\n",
        "                            break\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                # Extract individual item time (if different from auction)\n",
        "                item_end_time = auction_end_time\n",
        "                item_time_remaining = auction_time_remaining\n",
        "\n",
        "                # Look for time info within the lot element\n",
        "                time_elem = lot_elem.find(text=re.compile(r'End|Time.*Left|Closing', re.IGNORECASE))\n",
        "                if time_elem and time_elem.parent:\n",
        "                    time_text = time_elem.parent.get_text()\n",
        "                    end_time, time_remaining = self.parse_time_remaining(time_text)\n",
        "                    if end_time:\n",
        "                        item_end_time = end_time\n",
        "                        item_time_remaining = time_remaining\n",
        "\n",
        "                # Extract images - this is the key improvement\n",
        "                image_urls = self.extract_images(lot_elem, catalog_url)\n",
        "\n",
        "                # If no images found in lot element, try to find images by lot number\n",
        "                if not image_urls:\n",
        "                    # Look for images with lot number in src, alt, or nearby text\n",
        "                    all_images = soup.find_all('img')\n",
        "                    for img in all_images:\n",
        "                        img_src = img.get('src', '')\n",
        "                        img_alt = img.get('alt', '')\n",
        "                        img_class = ' '.join(img.get('class', []))\n",
        "\n",
        "                        # Check if image is related to this lot\n",
        "                        if (lot_number in img_src or\n",
        "                            lot_number in img_alt or\n",
        "                            lot_number in img_class):\n",
        "                            full_url = urljoin(catalog_url, img_src)\n",
        "                            if self.is_valid_image_url(full_url):\n",
        "                                image_urls.append(full_url)\n",
        "\n",
        "                item = AuctionItem(\n",
        "                    lot_number=lot_number,\n",
        "                    description=description,\n",
        "                    current_price=current_price,\n",
        "                    price_text=price_text,\n",
        "                    bid_count=bid_count,\n",
        "                    source=catalog_url,\n",
        "                    auction_id=auction_id,\n",
        "                    end_time=item_end_time,\n",
        "                    time_remaining=item_time_remaining,\n",
        "                    image_urls=image_urls,\n",
        "                    auction_title=auction_title,\n",
        "                    company_name=company_name\n",
        "                )\n",
        "\n",
        "                items.append(item)\n",
        "                logger.info(f\"Extracted lot {lot_number}: {description[:50]}... (${current_price if current_price else 'N/A'}) - {len(image_urls)} images\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error parsing lot {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(items)} items from {catalog_url}\")\n",
        "        return items\n",
        "\n",
        "    def scrape_and_store_all(self, include_individual_auctions=True, max_auctions=5):\n",
        "        \"\"\"Main method to scrape and store all items in database\"\"\"\n",
        "        logger.info(f\"Starting enhanced HiBid scraper for {'zip code ' + self.zip_code if self.zip_code else 'Indiana'}...\")\n",
        "\n",
        "        all_items = []\n",
        "        all_auctions = []\n",
        "\n",
        "        # Scrape zip code specific auctions if zip code provided\n",
        "        if self.zip_code:\n",
        "            zip_auctions = self.scrape_zip_code_auctions_enhanced(max_auctions * 2)\n",
        "            all_auctions.extend(zip_auctions)\n",
        "            logger.info(f\"Found {len(zip_auctions)} auctions near zip code {self.zip_code}\")\n",
        "\n",
        "        # Scrape individual auction pages\n",
        "        if include_individual_auctions:\n",
        "            auction_items = self.scrape_individual_auctions_enhanced(max_auctions)\n",
        "            all_items.extend(auction_items)\n",
        "            logger.info(f\"Found {len(auction_items)} items from individual auctions\")\n",
        "\n",
        "        # Store in database\n",
        "        stored_auctions = 0\n",
        "        stored_items = 0\n",
        "\n",
        "        for auction in all_auctions:\n",
        "            try:\n",
        "                self.db.save_auction(auction)\n",
        "                stored_auctions += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error storing auction: {e}\")\n",
        "\n",
        "        for item in all_items:\n",
        "            try:\n",
        "                self.db.save_item(item)\n",
        "                stored_items += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error storing item: {e}\")\n",
        "\n",
        "        logger.info(f\"Stored {stored_auctions} auctions and {stored_items} items in database\")\n",
        "        return all_items, all_auctions\n",
        "\n",
        "    def scrape_zip_code_auctions_enhanced(self, max_auctions=10):\n",
        "        \"\"\"Enhanced zip code auction scraping\"\"\"\n",
        "        if not self.zip_code:\n",
        "            return []\n",
        "\n",
        "        content = self.get_page_content(self.zip_url)\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        auctions = []\n",
        "\n",
        "        company_links = soup.find_all('a', href=re.compile(r'/company/\\d+/'))\n",
        "\n",
        "        for link in company_links[:max_auctions]:\n",
        "            try:\n",
        "                company_name = link.get_text().strip()\n",
        "                company_url = urljoin(self.base_url, link['href'])\n",
        "                auction_id = self.extract_auction_id(company_url)\n",
        "\n",
        "                parent = link.parent\n",
        "                while parent and parent.name != 'body':\n",
        "                    parent_text = parent.get_text()\n",
        "\n",
        "                    # Extract auction dates\n",
        "                    date_match = re.search(r'Date\\(s\\)\\s+([\\d/\\-\\s]+)', parent_text)\n",
        "                    dates = date_match.group(1).strip() if date_match else \"Unknown\"\n",
        "\n",
        "                    # Extract location\n",
        "                    map_link = parent.find('a', href=re.compile(r'google\\.com/maps'))\n",
        "                    location = \"Unknown\"\n",
        "                    if map_link and 'query=' in map_link['href']:\n",
        "                        location_query = map_link['href'].split('query=')[1]\n",
        "                        location = location_query.replace('%2C', ',').replace('%20', ' ')[:100]\n",
        "\n",
        "                    # Extract auction title\n",
        "                    title_elem = parent.find(['h2', 'h3', 'h4'])\n",
        "                    auction_title = title_elem.get_text().strip() if title_elem else company_name\n",
        "\n",
        "                    # Extract time information\n",
        "                    end_time, time_remaining = None, None\n",
        "                    time_text = parent.find(text=re.compile(r'End|Closing|Time.*Left', re.IGNORECASE))\n",
        "                    if time_text:\n",
        "                        end_time, time_remaining = self.parse_time_remaining(time_text.parent.get_text())\n",
        "\n",
        "                    # Extract bidding notice\n",
        "                    bidding_notice = \"\"\n",
        "                    notice_elem = parent.find(text=re.compile(r'Bidding Notice:|Auction Notice:'))\n",
        "                    if notice_elem:\n",
        "                        notice_parent = notice_elem.parent\n",
        "                        if notice_parent:\n",
        "                            bidding_notice = notice_parent.get_text().strip()[:200]\n",
        "\n",
        "                    auction = AuctionInfo(\n",
        "                        company_name=company_name,\n",
        "                        company_url=company_url,\n",
        "                        auction_title=auction_title,\n",
        "                        dates=dates,\n",
        "                        location=location,\n",
        "                        bidding_notice=bidding_notice,\n",
        "                        zip_code=self.zip_code,\n",
        "                        end_time=end_time,\n",
        "                        time_remaining=time_remaining,\n",
        "                        auction_id=auction_id\n",
        "                    )\n",
        "\n",
        "                    auctions.append(auction)\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error parsing auction info: {e}\")\n",
        "                continue\n",
        "\n",
        "        return auctions\n",
        "\n",
        "    def scrape_individual_auctions_enhanced(self, max_auctions=5):\n",
        "        \"\"\"Enhanced individual auction scraping\"\"\"\n",
        "        # Use zip code URL if available, otherwise use main Indiana URL\n",
        "        if self.zip_code:\n",
        "            content = self.get_page_content(self.zip_url)\n",
        "        else:\n",
        "            content = self.get_page_content(self.indiana_url)\n",
        "\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        auction_links = []\n",
        "\n",
        "        # Find auction links with improved patterns\n",
        "        link_patterns = [\n",
        "            r'/catalog/\\d+',\n",
        "            r'/auction/\\d+',\n",
        "            r'/auctions/\\d+',\n",
        "            r'/sale/\\d+'\n",
        "        ]\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            for pattern in link_patterns:\n",
        "                if re.search(pattern, href):\n",
        "                    full_url = urljoin(self.base_url, href)\n",
        "                    auction_links.append(full_url)\n",
        "                    break\n",
        "\n",
        "        # Remove duplicates and limit\n",
        "        auction_links = list(set(auction_links))[:max_auctions]\n",
        "\n",
        "        all_items = []\n",
        "        for auction_url in auction_links:\n",
        "            logger.info(f\"Scraping auction: {auction_url}\")\n",
        "            items = self.scrape_catalog_page_enhanced(auction_url)\n",
        "            all_items.extend(items)\n",
        "            time.sleep(2)  # Be more respectful with delays\n",
        "\n",
        "        return all_items\n",
        "\n",
        "# Web Application Helper Functions\n",
        "def get_recent_auctions(db_path='hibid_auctions.db', zip_code=None, hours=24):\n",
        "    \"\"\"Get recent auctions from database\"\"\"\n",
        "    db = DatabaseManager(db_path)\n",
        "    return db.get_active_auctions(zip_code)\n",
        "\n",
        "def get_auction_items(auction_id, db_path='hibid_auctions.db'):\n",
        "    \"\"\"Get all items for a specific auction\"\"\"\n",
        "    db = DatabaseManager(db_path)\n",
        "    return db.get_items_by_auction(auction_id)\n",
        "\n",
        "def search_items(query, db_path='hibid_auctions.db', limit=50):\n",
        "    \"\"\"Search for items by description\"\"\"\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT i.*, GROUP_CONCAT(img.image_url) as image_urls\n",
        "            FROM items i\n",
        "            LEFT JOIN item_images img ON i.id = img.item_id\n",
        "            WHERE i.description LIKE ?\n",
        "            AND datetime(i.scraped_at) > datetime('now', '-24 hours')\n",
        "            GROUP BY i.id\n",
        "            ORDER BY i.current_price DESC\n",
        "            LIMIT ?\n",
        "        ''', (f'%{query}%', limit))\n",
        "\n",
        "        columns = [desc[0] for desc in cursor.description]\n",
        "        items = []\n",
        "        for row in cursor.fetchall():\n",
        "            item = dict(zip(columns, row))\n",
        "            if item['image_urls']:\n",
        "                item['image_urls'] = item['image_urls'].split(',')\n",
        "            else:\n",
        "                item['image_urls'] = []\n",
        "            items.append(item)\n",
        "        return items\n",
        "\n",
        "def validate_images(db_path='hibid_auctions.db'):\n",
        "    \"\"\"Validate and test image URLs in database\"\"\"\n",
        "    logger.info(\"Validating image URLs...\")\n",
        "\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('SELECT DISTINCT image_url FROM item_images LIMIT 10')\n",
        "\n",
        "        session = requests.Session()\n",
        "        session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "        for (url,) in cursor.fetchall():\n",
        "            try:\n",
        "                response = session.head(url, timeout=5)\n",
        "                status = response.status_code\n",
        "                content_type = response.headers.get('content-type', '')\n",
        "\n",
        "                logger.info(f\"URL: {url}\")\n",
        "                logger.info(f\"  Status: {status}\")\n",
        "                logger.info(f\"  Content-Type: {content_type}\")\n",
        "                logger.info(f\"  Valid: {'Yes' if status == 200 and 'image' in content_type else 'No'}\")\n",
        "                logger.info(\"-\" * 50)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error checking {url}: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the enhanced scraper\"\"\"\n",
        "    zip_code = \"46074\"  # Change this to your desired zip code\n",
        "    scraper = EnhancedHiBidScraper(zip_code=zip_code)\n",
        "\n",
        "    try:\n",
        "        # Scrape and store all items\n",
        "        items, auctions = scraper.scrape_and_store_all(\n",
        "            include_individual_auctions=True,\n",
        "            max_auctions=3\n",
        "        )\n",
        "\n",
        "        if items:\n",
        "            logger.info(f\"Total items found and stored: {len(items)}\")\n",
        "\n",
        "            # Show sample items with enhanced info\n",
        "            for i, item in enumerate(items[:3]):\n",
        "                logger.info(f\"\\n{i+1}. Lot {item.lot_number}\")\n",
        "                logger.info(f\"   Description: {item.description[:100]}...\")\n",
        "                logger.info(f\"   Price: ${item.current_price:.2f}\" if item.current_price else \"   Price: Not available\")\n",
        "                logger.info(f\"   Bids: {item.bid_count}\")\n",
        "                logger.info(f\"   Time Remaining: {item.time_remaining}\")\n",
        "                logger.info(f\"   Images: {len(item.image_urls)} found\")\n",
        "                logger.info(f\"   Company: {item.company_name}\")\n",
        "                if item.image_urls:\n",
        "                    logger.info(f\"   First Image: {item.image_urls[0]}\")\n",
        "\n",
        "        # Demonstrate database queries\n",
        "        logger.info(\"\\n\" + \"=\"*50)\n",
        "        logger.info(\"Database Query Examples:\")\n",
        "\n",
        "        # Get recent auctions\n",
        "        recent_auctions = get_recent_auctions(zip_code=zip_code)\n",
        "        logger.info(f\"Recent auctions found: {len(recent_auctions)}\")\n",
        "\n",
        "        # Search for specific items\n",
        "        search_results = search_items(\"furniture\")\n",
        "        logger.info(f\"Furniture items found: {len(search_results)}\")\n",
        "\n",
        "        # Validate some image URLs\n",
        "        logger.info(\"\\nValidating image URLs:\")\n",
        "        validate_images()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error running enhanced scraper: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    main()"
      ],
      "metadata": {
        "id": "kdUMF5cfjVtQ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_code = \"46074\"  # Change this to your desired zip code\n",
        "scraper = EnhancedHiBidScraper(zip_code=zip_code)\n",
        "\n",
        "\n",
        "# Scrape and store all items\n",
        "items, auctions = scraper.scrape_and_store_all(\n",
        "    include_individual_auctions=True,\n",
        "    max_auctions=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "jjskHI0XkOA9",
        "outputId": "a9200baf-7207-4893-ac6b-39c22d5cf157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-e26e3098fe20>:816: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  time_text = parent.find(text=re.compile(r'End|Closing|Time.*Left', re.IGNORECASE))\n",
            "<ipython-input-64-e26e3098fe20>:822: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  notice_elem = parent.find(text=re.compile(r'Bidding Notice:|Auction Notice:'))\n",
            "<ipython-input-64-e26e3098fe20>:682: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  time_elem = lot_elem.find(text=re.compile(r'End|Time.*Left|Closing', re.IGNORECASE))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items[10]"
      ],
      "metadata": {
        "id": "96qwIlc_kUgu",
        "outputId": "d72f5af4-3bc6-44f5-f1af-0e07360d5c93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuctionItem(lot_number='6', description='No description', current_price=2.0, price_text='USD \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    2', bid_count=2, source='https://hibid.com/indiana/catalog/647441/new-stock--collectibles--vintage-advertising-and-more-', auction_id='647441', end_time=None, time_remaining=None, image_urls=['https://cdn.hibid.com/img.axd?id=8144170162&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=AG0Ek%2bQorteSx8LpvvujfQ9NJgYpdeWl', 'https://cdn.hibid.com/img.axd?id=8144168099&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=dMg7g%2ffNbPGCM7ysaAqD%2fumujEANWHT3&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168118&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGUOPVyJJhlOtWC7o3M%2bMe6V&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168130&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGWZSntXfA3v%2f154u%2bgC7cHu&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168143&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGVZVLGWT13V9sx3i7PoWLQX&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168161&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGWE%2bnFgeaiOtjF0dGPpgYxa&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168177&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGUd7ZXcxCm3q5FleK4ddsqy&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168190&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGW%2baxDT1L0%2bXoYjuV%2bkP%2fB7&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168195&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=3ZHoBuFIjGVDjjHEnhvmxPwQI7vi73fD&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168212&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GG3TvFq9ofh1DVoSDY%2bHiLdZ56oEuB7q&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168231&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GG3TvFq9ofh10%2fdYXzJFZQbdJhQvCKSa&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168246&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GG3TvFq9ofhf8llZwYkqO547PifsY%2fNh&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168267&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GG3TvFq9ofhH9FWlFEi6fBYw4oCY6UY2&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168274&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GG3TvFq9ofgxvO2Mi%2ftlvetot%2fxZI%2fLq&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168305&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIGf%2bZ4D0TcbXyWpqyPmL1I%2f&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168351&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIEGkaWKr4ewlN4sy0erIOW6&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168358&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIGCGzP%2b6xgZKMwA9h7Laywt&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168373&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIFABVviTsEv4cwq1JmftGS%2b&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168383&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIEI9Mq4X8Y8Vs0QXiy4sP0z&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168395&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=lK9KTK2SvIHUnk0RNnb3QRy7YhBalsYY&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168411&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soSxdk9sU0wsLN8UnAKXu7OY&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168436&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soTNPkqqQzMCaC5KJdDtiPy9&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168444&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soR5LwC0ci4%2f2L2f86JyvKPs&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168464&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soSKcLUQ3KTD9F2TTl31082Z&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168476&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soREfxrauBfJ%2fLs7fAdylfSV&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168491&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=MBrcg0q8soS90xCN6KZb5YU6ghEyTP1q&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168525&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhwSRjBM9Pg1CDc3UN5z%2flUH&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168539&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhx2T%2fDSOVILbw6uKSrr06yA&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168558&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhyAugA2hNAzBa7pSL3yYLmV&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168573&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhwwyAGwHPzRx%2fU7YDfJ%2bCgo&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168588&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhxJeNKIeBGbrT5OZ4K6hjgq&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168595&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Tla9G9foPhwQ8ukIqboGoyPYRkNZoHHN&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168611&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=0R%2f1Qce0SD5jt16Vv5L%2fVs%2fgGLbzMW0T&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168631&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=0R%2f1Qce0SD4LYPwGt0wGlwHeCoLVax08&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168645&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=0R%2f1Qce0SD7SaQHGF%2bXPGuZx8AX9M0uz&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168665&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=0R%2f1Qce0SD6EmbCPbctxdlZjdXaGO0dy&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168687&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=0R%2f1Qce0SD7VGG9hG9PN05TDacnI8m3k&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168704&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm1tZ2fk1k2wmePfMH5wMEdf&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168711&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm15JLOloUZFLTT8ggyL6SGx&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168734&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm2AYU0igs7IdLpgQe1xF6e5&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168751&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm1J8d90MeGm6Z%2fmN%2fJE05wn&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168758&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm3k%2fl0tMzPqTVnhXBLk01xv&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168776&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm2xY0DWGJ9IYEM2GXr8dBfJ&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168794&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=BqwRbzh8Jm2Is57ItD96%2bNKto4gmSxts&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168809&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=YygNXIPqKbtJCov%2bpWPhOSME4CVz4%2fKh&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168827&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=YygNXIPqKbsxyOVERowu5bH7OcAFq%2bsZ&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168839&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=YygNXIPqKbvklroecRBv%2fdFpgKUrB0ff&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168872&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=YygNXIPqKbvOP54iljYYg6ns1JKrcwib&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168895&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=YygNXIPqKbtLB%2bPdORvbdD6PiIOhZw86&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168902&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRregaqbQO4IGTY53oZ8TD5BX&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168918&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRre%2ft8HQ%2fFuJx2M9r8%2fqJQjY&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168934&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRrcM7fDULULYk8zX13YekkH4&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168946&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRrfuH16h5DIHGIv0JTdlTVs8&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168965&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRrec8I3AsLWpw7TnR9%2fQmLeI&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144168986&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=N8NXZdoBRreXsK72KEvDENFTEjuJpXfK&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169001&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=R0OL%2bmpVkOdsIjxGztsFH%2fcRx6ZLs02x&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169021&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=R0OL%2bmpVkOdsMkswzn7mG9whfXk1suc0&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169039&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=R0OL%2bmpVkOfavt%2fxfcDEUiGbyrlyrF%2fO&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169080&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=R0OL%2bmpVkOcNK%2fjvDf9QimRLzagM3rWo&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169092&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=R0OL%2bmpVkOe89QqXBavBKIOAgTgBBV0a&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169104&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=u9SVXAiHY6iVaO5M6%2fKzCbnYbo7xq5S9&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169121&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=u9SVXAiHY6gCNGJ9Zq1LkEbK5UEfDr5Q&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169136&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=u9SVXAiHY6huiPoqXmrq%2fdQlsUis8yjs&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169161&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=u9SVXAiHY6jJDJgJ3Z3lGeaLKzIHz7sX&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169190&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=u9SVXAiHY6gB7fIz%2fYA5uCLB1qgYqbSF&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169207&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62sIjEv90hnXqvUeuvvQAuld&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169226&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62tA10p8b86QXf1BJJ7EGp4j&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169243&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62so7xoNcy7Xnhcu1fF3w98f&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169275&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62tNHKVJd%2fqMgP8I0sB1J%2boU&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169284&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62t7Fug9hVR8dXUn60EWGANz&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169299&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=oPkW0oMR62sF439wqMumLXdEglboj%2f%2bg&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169313&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ80pb0pBlFJ679B3o8l%2fFPsP&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169325&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ8079Bj%2b8RwK9dvQ02BO%2f5%2f0&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169353&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ81pmCHVvcvi6%2fbMtvBi8gWx&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169380&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ83TNscICibyj9hsasd6PmKc&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169386&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ81yHWEnpRht%2bf5LsZibv5V%2f&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169392&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=CL4KoPNoJ82OQpiz%2f62E1njIV7fkQ%2bWV&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169420&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=eelt4SN1wiQtgs60y%2bwczCp6ZjyUdNAR&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169435&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=eelt4SN1wiSEZ0THnL%2fEtnNKFjm8BBTL&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169455&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=eelt4SN1wiStdreKm3Cak53GLg%2f4KoF5&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169473&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=eelt4SN1wiQE1xVserb2qbQchXXnFsou&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169488&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=eelt4SN1wiQt6J4PNNZsL5%2fONXMsWpJa&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169502&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=7%2fl4kuwRCmCnyAVgpfZ9FQt6mI5J7ujU&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169524&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=7%2fl4kuwRCmAOC5hQilD3qRKamOmioiV7&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169531&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=7%2fl4kuwRCmDHZ9%2fTBLUJIvLJ4jkNtADf&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169562&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=7%2fl4kuwRCmBJEfabklTlkdUcEv3PvdzZ&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169586&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=7%2fl4kuwRCmC%2bpH%2fYGGnQIKNIQgad6Qta&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169602&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Ti9nCPuviX1WQ6yC%2fLVuU%2fGf5wXvgZWq&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169616&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Ti9nCPuviX3GFE8wWerf1syfGPD9kwiB&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169635&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Ti9nCPuviX1c1jsUtSPsrtQWCrkxTiIj&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169655&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Ti9nCPuviX2OVeNYFSBmZYfwiXNAtXyt&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169674&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=Ti9nCPuviX00Ci99yNgVDBsrIdJeKpto&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169705&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GSgrAlVG0v8wsI32bJX3NeuYIw9GUSj1&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169727&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GSgrAlVG0v%2beAQ7DGmqcWjC87%2f%2bgSVqa&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169747&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GSgrAlVG0v8VfYPIsYrfWssw1zu8gpF%2f&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169767&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GSgrAlVG0v8jn%2fpGH3l8lIs0X7vgm01C&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169782&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=GSgrAlVG0v%2buaJ3cogsj3y54h47voPGq&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169813&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=H15O2FpHhg0dPNgAPslLywckicJzj2wd&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169954&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=aBsnnreauRgce0PEFpOnrTRCAlxy7sVF&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169972&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=aBsnnreauRhp1o%2fr3Druc5GlZ53xYDj4&h=200&w=200', 'https://cdn.hibid.com/img.axd?id=8144169984&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=aBsnnreauRjzatetLdGUSsiASLkXqBiF&h=200&w=200', 'https://seal-nebraska.bbb.org/logo/rbhzbus/hibid-auctions-300282673.png'], auction_title='New Stock, Collectibles, Vintage Advertising & More!\\n\\n\\n\\n\\n\\tOnline Only Auction', company_name='One Stop Auction Shop', scraped_at='2025-06-04T20:47:35.968321')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Earl's scraper"
      ],
      "metadata": {
        "id": "KdcTGj02jXMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Earl's Auction Scraper - Fixed Version with Robust Price Parsing\n",
        "Complete working scraper with all the price parsing issues resolved\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import sqlite3\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, List, Dict\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up debug logging to see what's happening with price parsing\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set to DEBUG to see detailed price parsing info\n",
        "# logging.getLogger(__name__).setLevel(logging.DEBUG)\n",
        "\n",
        "@dataclass\n",
        "class AuctionItem:\n",
        "    lot_number: str\n",
        "    description: str\n",
        "    current_price: Optional[float]\n",
        "    price_text: str\n",
        "    bid_count: int\n",
        "    source: str\n",
        "    auction_id: Optional[str] = None\n",
        "    end_time: Optional[str] = None\n",
        "    time_remaining: Optional[str] = None\n",
        "    image_urls: List[str] = None\n",
        "    auction_title: Optional[str] = None\n",
        "    company_name: Optional[str] = None\n",
        "    scraped_at: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.image_urls is None:\n",
        "            self.image_urls = []\n",
        "        if self.scraped_at is None:\n",
        "            self.scraped_at = datetime.now().isoformat()\n",
        "\n",
        "@dataclass\n",
        "class AuctionInfo:\n",
        "    company_name: str\n",
        "    company_url: str\n",
        "    auction_title: str\n",
        "    dates: str\n",
        "    location: str\n",
        "    bidding_notice: str\n",
        "    zip_code: Optional[str]\n",
        "    end_time: Optional[str] = None\n",
        "    time_remaining: Optional[str] = None\n",
        "    auction_id: Optional[str] = None\n",
        "    scraped_at: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.scraped_at is None:\n",
        "            self.scraped_at = datetime.now().isoformat()\n",
        "\n",
        "class DatabaseManager:\n",
        "    def __init__(self, db_path='hibid_auctions.db'):\n",
        "        self.db_path = db_path\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Initialize the database with required tables\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            # Auctions table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS auctions (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    auction_id TEXT UNIQUE,\n",
        "                    company_name TEXT,\n",
        "                    company_url TEXT,\n",
        "                    auction_title TEXT,\n",
        "                    dates TEXT,\n",
        "                    location TEXT,\n",
        "                    bidding_notice TEXT,\n",
        "                    zip_code TEXT,\n",
        "                    end_time TEXT,\n",
        "                    time_remaining TEXT,\n",
        "                    scraped_at TEXT,\n",
        "                    UNIQUE(auction_id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Items table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS items (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    lot_number TEXT,\n",
        "                    description TEXT,\n",
        "                    current_price REAL,\n",
        "                    price_text TEXT,\n",
        "                    bid_count INTEGER,\n",
        "                    source TEXT,\n",
        "                    auction_id TEXT,\n",
        "                    end_time TEXT,\n",
        "                    time_remaining TEXT,\n",
        "                    auction_title TEXT,\n",
        "                    company_name TEXT,\n",
        "                    scraped_at TEXT,\n",
        "                    FOREIGN KEY (auction_id) REFERENCES auctions (auction_id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Images table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS item_images (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    item_id INTEGER,\n",
        "                    image_url TEXT,\n",
        "                    image_order INTEGER,\n",
        "                    FOREIGN KEY (item_id) REFERENCES items (id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            conn.commit()\n",
        "\n",
        "    def save_auction(self, auction: AuctionInfo) -> int:\n",
        "        \"\"\"Save auction info to database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                INSERT OR REPLACE INTO auctions\n",
        "                (auction_id, company_name, company_url, auction_title, dates,\n",
        "                 location, bidding_notice, zip_code, end_time, time_remaining, scraped_at)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                auction.auction_id, auction.company_name, auction.company_url,\n",
        "                auction.auction_title, auction.dates, auction.location,\n",
        "                auction.bidding_notice, auction.zip_code, auction.end_time,\n",
        "                auction.time_remaining, auction.scraped_at\n",
        "            ))\n",
        "            return cursor.lastrowid\n",
        "\n",
        "    def save_item(self, item: AuctionItem) -> int:\n",
        "        \"\"\"Save auction item to database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                INSERT INTO items\n",
        "                (lot_number, description, current_price, price_text, bid_count,\n",
        "                 source, auction_id, end_time, time_remaining, auction_title,\n",
        "                 company_name, scraped_at)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                item.lot_number, item.description, item.current_price,\n",
        "                item.price_text, item.bid_count, item.source, item.auction_id,\n",
        "                item.end_time, item.time_remaining, item.auction_title,\n",
        "                item.company_name, item.scraped_at\n",
        "            ))\n",
        "\n",
        "            item_id = cursor.lastrowid\n",
        "\n",
        "            # Save images\n",
        "            for i, image_url in enumerate(item.image_urls):\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO item_images (item_id, image_url, image_order)\n",
        "                    VALUES (?, ?, ?)\n",
        "                ''', (item_id, image_url, i))\n",
        "\n",
        "            return item_id\n",
        "\n",
        "    def get_active_auctions(self, zip_code: Optional[str] = None) -> List[Dict]:\n",
        "        \"\"\"Get active auctions from database\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            query = '''\n",
        "                SELECT * FROM auctions\n",
        "                WHERE datetime(scraped_at) > datetime('now', '-24 hours')\n",
        "            '''\n",
        "            params = []\n",
        "\n",
        "            if zip_code:\n",
        "                query += ' AND zip_code = ?'\n",
        "                params.append(zip_code)\n",
        "\n",
        "            query += ' ORDER BY scraped_at DESC'\n",
        "\n",
        "            cursor.execute(query, params)\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "\n",
        "    def get_items_by_auction(self, auction_id: str) -> List[Dict]:\n",
        "        \"\"\"Get all items for a specific auction\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "                SELECT i.*, GROUP_CONCAT(img.image_url) as image_urls\n",
        "                FROM items i\n",
        "                LEFT JOIN item_images img ON i.id = img.item_id\n",
        "                WHERE i.auction_id = ?\n",
        "                GROUP BY i.id\n",
        "                ORDER BY CAST(i.lot_number AS INTEGER)\n",
        "            ''', (auction_id,))\n",
        "\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            items = []\n",
        "            for row in cursor.fetchall():\n",
        "                item = dict(zip(columns, row))\n",
        "                if item['image_urls']:\n",
        "                    item['image_urls'] = item['image_urls'].split(',')\n",
        "                else:\n",
        "                    item['image_urls'] = []\n",
        "                items.append(item)\n",
        "            return items\n",
        "\n",
        "class EarlsAuctionScraper:\n",
        "    \"\"\"Earl's Auction scraper with fixed price parsing\"\"\"\n",
        "\n",
        "    def __init__(self, zip_code=None, db_path='hibid_auctions.db'):\n",
        "        self.base_url = \"https://www.earlsauction.com\"\n",
        "        self.auctions_url = \"https://www.earlsauction.com/auctions\"\n",
        "        self.zip_code = zip_code\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "        self.db = DatabaseManager(db_path)\n",
        "\n",
        "    def get_page_content(self, url, retries=3):\n",
        "        \"\"\"Fetch page content with retry logic\"\"\"\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                return response.text\n",
        "            except requests.RequestException as e:\n",
        "                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def parse_price(self, price_text):\n",
        "        \"\"\"Extract numeric price from price text - FIXED VERSION\"\"\"\n",
        "        if not price_text:\n",
        "            return None\n",
        "\n",
        "        logger.debug(f\"Parsing price from: {repr(price_text[:200])}\")\n",
        "\n",
        "        # Clean up the text first - remove excessive whitespace\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', price_text.strip())\n",
        "\n",
        "        # Strategy 1: Look for very specific Earl's auction patterns\n",
        "        earls_specific_patterns = [\n",
        "            r'Current Bid:\\s*\\$(\\d{1,4}(?:\\.\\d{2})?)',  # Current Bid: $55.00\n",
        "            r'Winning Bid:\\s*\\$(\\d{1,4}(?:\\.\\d{2})?)',  # Winning Bid: $55.00\n",
        "            r'High Bid:\\s*\\$(\\d{1,4}(?:\\.\\d{2})?)',     # High Bid: $55.00\n",
        "            r'Starting Bid:\\s*\\$(\\d{1,4}(?:\\.\\d{2})?)', # Starting Bid: $5.00\n",
        "        ]\n",
        "\n",
        "        for pattern in earls_specific_patterns:\n",
        "            match = re.search(pattern, cleaned_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    price = float(match.group(1))\n",
        "                    logger.debug(f\"Found price using pattern {pattern}: ${price}\")\n",
        "                    if 0.01 <= price <= 50000:  # Reasonable range\n",
        "                        return price\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        # Strategy 2: Look for standalone dollar amounts but be VERY restrictive\n",
        "        standalone_patterns = [\n",
        "            r'(?<!\\d)\\$(\\d{1,4}(?:\\.\\d{2})?)(?!\\d)',  # $55.00 not part of larger number\n",
        "        ]\n",
        "\n",
        "        all_found_prices = []\n",
        "        for pattern in standalone_patterns:\n",
        "            matches = re.finditer(pattern, cleaned_text)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    price = float(match.group(1))\n",
        "                    logger.debug(f\"Found potential price: ${price}\")\n",
        "                    if 0.01 <= price <= 50000:\n",
        "                        all_found_prices.append(price)\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        if all_found_prices:\n",
        "            best_price = max(all_found_prices)\n",
        "            logger.debug(f\"Selected best price from {all_found_prices}: ${best_price}\")\n",
        "            return best_price\n",
        "\n",
        "        logger.debug(f\"Could not parse valid price from: {repr(price_text[:100])}\")\n",
        "        return None\n",
        "\n",
        "    def parse_time_remaining(self, time_text):\n",
        "        \"\"\"Parse time remaining from various formats\"\"\"\n",
        "        if not time_text:\n",
        "            return None, None\n",
        "\n",
        "        time_text = re.sub(r'\\s+', ' ', time_text.strip())\n",
        "\n",
        "        patterns = [\n",
        "            r'(\\d+)d\\s*(\\d+)h\\s*(\\d+)m',\n",
        "            r'(\\d+)\\s*days?\\s*(\\d+)\\s*hours?\\s*(\\d+)\\s*min',\n",
        "            r'(\\d+)h\\s*(\\d+)m',\n",
        "            r'(\\d+)\\s*hours?\\s*(\\d+)\\s*min',\n",
        "            r'(\\d+)m',\n",
        "            r'(\\d+)\\s*min',\n",
        "            r'Ends?:?\\s*(.+)',\n",
        "            r'Closing:?\\s*(.+)',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, time_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                groups = match.groups()\n",
        "\n",
        "                if len(groups) == 3:\n",
        "                    try:\n",
        "                        days, hours, minutes = map(int, groups)\n",
        "                        end_time = datetime.now() + timedelta(days=days, hours=hours, minutes=minutes)\n",
        "                        return end_time.isoformat(), f\"{days}d {hours}h {minutes}m\"\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                elif len(groups) == 2 and any(x in time_text.lower() for x in ['h', 'hour']):\n",
        "                    try:\n",
        "                        hours, minutes = map(int, groups)\n",
        "                        end_time = datetime.now() + timedelta(hours=hours, minutes=minutes)\n",
        "                        return end_time.isoformat(), f\"{hours}h {minutes}m\"\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                elif len(groups) == 1:\n",
        "                    if any(x in time_text.lower() for x in ['m', 'min']):\n",
        "                        try:\n",
        "                            minutes = int(groups[0])\n",
        "                            end_time = datetime.now() + timedelta(minutes=minutes)\n",
        "                            return end_time.isoformat(), f\"{minutes}m\"\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "        return None, time_text\n",
        "\n",
        "    def is_valid_image_url(self, url):\n",
        "        \"\"\"Check if URL is a valid image URL\"\"\"\n",
        "        if not url:\n",
        "            return False\n",
        "\n",
        "        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        if any(ext in url_lower for ext in image_extensions):\n",
        "            return True\n",
        "\n",
        "        image_keywords = ['image', 'img', 'photo', 'picture', 'thumb', 'gallery']\n",
        "        if any(keyword in url_lower for keyword in image_keywords):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def extract_images(self, soup, base_url):\n",
        "        \"\"\"Extract images from page\"\"\"\n",
        "        images = []\n",
        "        all_imgs = soup.find_all('img')\n",
        "\n",
        "        for img in all_imgs:\n",
        "            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')\n",
        "            if not src:\n",
        "                continue\n",
        "\n",
        "            full_url = urljoin(base_url, src)\n",
        "\n",
        "            skip_patterns = [\n",
        "                'logo', 'banner', 'header', 'footer', 'icon',\n",
        "                'avatar', 'profile', 'social', 'ad', 'advertisement',\n",
        "                'placeholder', 'loading', 'spinner', '1x1', 'tracking'\n",
        "            ]\n",
        "\n",
        "            if any(pattern in full_url.lower() for pattern in skip_patterns):\n",
        "                continue\n",
        "\n",
        "            if self.is_valid_image_url(full_url):\n",
        "                width = img.get('width')\n",
        "                height = img.get('height')\n",
        "\n",
        "                if width and height:\n",
        "                    try:\n",
        "                        w, h = int(width), int(height)\n",
        "                        if w < 50 or h < 50:\n",
        "                            continue\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "                img_classes = img.get('class', [])\n",
        "                if isinstance(img_classes, str):\n",
        "                    img_classes = img_classes.split()\n",
        "\n",
        "                priority_classes = ['lot', 'item', 'product', 'auction', 'photo', 'image', 'gallery']\n",
        "                has_priority = any(cls.lower() in ' '.join(img_classes).lower() for cls in priority_classes)\n",
        "\n",
        "                if full_url not in images:\n",
        "                    if has_priority:\n",
        "                        images.insert(0, full_url)\n",
        "                    else:\n",
        "                        images.append(full_url)\n",
        "\n",
        "        cleaned_images = []\n",
        "        for img_url in images[:10]:\n",
        "            parsed = urlparse(img_url)\n",
        "            if parsed.scheme and parsed.netloc:\n",
        "                cleaned_images.append(img_url)\n",
        "\n",
        "        return cleaned_images[:5]\n",
        "\n",
        "    def extract_auction_id(self, url):\n",
        "        \"\"\"Extract auction ID from URL\"\"\"\n",
        "        patterns = [\n",
        "            r'/auction/(\\d+)',\n",
        "            r'/catalog/(\\d+)',\n",
        "            r'/company/(\\d+)',\n",
        "            r'auction_id=(\\d+)',\n",
        "            r'/auctions/(\\d+)',\n",
        "            r'/sale/(\\d+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, url)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def create_auction_item_safely(self, lot_number, description, current_price, price_text, bid_count, source, auction_id, end_time, time_remaining, image_urls, auction_title, company_name):\n",
        "        \"\"\"Create AuctionItem with price validation\"\"\"\n",
        "\n",
        "        # Validate the price before creating the item\n",
        "        if current_price is not None:\n",
        "            if current_price < 0.01 or current_price > 100000:\n",
        "                logger.warning(f\"Invalid price ${current_price} for lot {lot_number}, setting to None\")\n",
        "                current_price = None\n",
        "                price_text = \"Price unavailable\"\n",
        "            elif current_price > 10000:\n",
        "                logger.warning(f\"High price ${current_price} for lot {lot_number} - please verify\")\n",
        "\n",
        "        return AuctionItem(\n",
        "            lot_number=lot_number,\n",
        "            description=description,\n",
        "            current_price=current_price,\n",
        "            price_text=price_text,\n",
        "            bid_count=bid_count,\n",
        "            source=source,\n",
        "            auction_id=auction_id,\n",
        "            end_time=end_time,\n",
        "            time_remaining=time_remaining,\n",
        "            image_urls=image_urls,\n",
        "            auction_title=auction_title,\n",
        "            company_name=company_name\n",
        "        )\n",
        "\n",
        "    def scrape_earls_auctions(self):\n",
        "        \"\"\"Main method to scrape Earl's auction listings\"\"\"\n",
        "        logger.info(\"Scraping Earl's Auction main page...\")\n",
        "\n",
        "        content = self.get_page_content(self.auctions_url)\n",
        "        if not content:\n",
        "            logger.warning(\"Could not fetch content from Earl's Auction\")\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        auctions = []\n",
        "\n",
        "        auction_urls = self._find_direct_auction_urls(soup)\n",
        "\n",
        "        if auction_urls:\n",
        "            logger.info(f\"Found {len(auction_urls)} direct auction URLs\")\n",
        "            for url in auction_urls:\n",
        "                try:\n",
        "                    auction = self._create_auction_from_url(url)\n",
        "                    if auction:\n",
        "                        auctions.append(auction)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error creating auction from URL {url}: {e}\")\n",
        "                    continue\n",
        "        else:\n",
        "            logger.info(\"No direct auction URLs found, trying to parse auction containers\")\n",
        "            auction_containers = self._find_auction_containers(soup)\n",
        "\n",
        "            for container in auction_containers:\n",
        "                try:\n",
        "                    auction = self._parse_auction_container(container)\n",
        "                    if auction:\n",
        "                        auctions.append(auction)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error parsing auction container: {e}\")\n",
        "                    continue\n",
        "\n",
        "        logger.info(f\"Found {len(auctions)} auctions on Earl's Auction\")\n",
        "        return auctions\n",
        "\n",
        "    def _find_direct_auction_urls(self, soup):\n",
        "        \"\"\"Find direct auction URLs on the main auction page\"\"\"\n",
        "        auction_urls = []\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "\n",
        "        for link in all_links:\n",
        "            href = link['href']\n",
        "            if re.search(r'/auctions/\\d+', href) and '/lot/' not in href:\n",
        "                full_url = urljoin(self.base_url, href)\n",
        "                auction_urls.append(full_url)\n",
        "\n",
        "        seen = set()\n",
        "        unique_urls = []\n",
        "        for url in auction_urls:\n",
        "            if url not in seen:\n",
        "                seen.add(url)\n",
        "                unique_urls.append(url)\n",
        "\n",
        "        return unique_urls\n",
        "\n",
        "    def _create_auction_from_url(self, auction_url):\n",
        "        \"\"\"Create an AuctionInfo object from a direct auction URL\"\"\"\n",
        "        try:\n",
        "            auction_id = self.extract_auction_id(auction_url)\n",
        "\n",
        "            url_match = re.search(r'/auctions/\\d+-(.+)', auction_url)\n",
        "            if url_match:\n",
        "                title_from_url = url_match.group(1).replace('-', ' ').title()\n",
        "            else:\n",
        "                title_from_url = \"Earl's Auction\"\n",
        "\n",
        "            auction_details = self._fetch_auction_details(auction_url)\n",
        "\n",
        "            auction = AuctionInfo(\n",
        "                company_name=\"Earl's Auction Company\",\n",
        "                company_url=auction_url,\n",
        "                auction_title=auction_details.get('title', title_from_url),\n",
        "                dates=auction_details.get('dates', 'TBD'),\n",
        "                location=auction_details.get('location', 'Indianapolis, IN'),\n",
        "                bidding_notice=auction_details.get('bidding_notice', ''),\n",
        "                zip_code=self.zip_code,\n",
        "                end_time=auction_details.get('end_time'),\n",
        "                time_remaining=auction_details.get('time_remaining'),\n",
        "                auction_id=auction_id\n",
        "            )\n",
        "\n",
        "            return auction\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating auction from URL {auction_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _fetch_auction_details(self, auction_url):\n",
        "        \"\"\"Fetch additional details from an auction page\"\"\"\n",
        "        details = {}\n",
        "\n",
        "        try:\n",
        "            content = self.get_page_content(auction_url)\n",
        "            if not content:\n",
        "                return details\n",
        "\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            title_tag = soup.find('title')\n",
        "            if title_tag:\n",
        "                title_text = title_tag.get_text().strip()\n",
        "                title_text = re.sub(r\"\\s*-\\s*Earl's Auction Company.*$\", '', title_text)\n",
        "                if len(title_text) > 5:\n",
        "                    details['title'] = title_text\n",
        "\n",
        "            page_text = soup.get_text()\n",
        "\n",
        "            pickup_match = re.search(r'(?:pickup|pick\\s*up).*?at\\s+([^.]+)', page_text, re.IGNORECASE)\n",
        "            if pickup_match:\n",
        "                location_text = pickup_match.group(1).strip()\n",
        "                if len(location_text) < 100:\n",
        "                    details['location'] = location_text\n",
        "\n",
        "            date_patterns = [\n",
        "                r'(\\w+,\\s+\\w+\\s+\\d+(?:th|st|nd|rd)?)',\n",
        "                r'(\\d{1,2}/\\d{1,2}/\\d{4})',\n",
        "                r'(\\w+\\s+\\d{1,2}(?:th|st|nd|rd)?,?\\s+\\d{4})'\n",
        "            ]\n",
        "\n",
        "            for pattern in date_patterns:\n",
        "                date_match = re.search(pattern, page_text)\n",
        "                if date_match:\n",
        "                    details['dates'] = date_match.group(1)\n",
        "                    break\n",
        "\n",
        "            time_keywords = ['ends', 'ending', 'closes', 'closing']\n",
        "            for keyword in time_keywords:\n",
        "                if keyword in page_text.lower():\n",
        "                    lines = page_text.split('\\n')\n",
        "                    for line in lines:\n",
        "                        if keyword in line.lower():\n",
        "                            end_time, time_remaining = self.parse_time_remaining(line)\n",
        "                            if end_time:\n",
        "                                details['end_time'] = end_time\n",
        "                                details['time_remaining'] = time_remaining\n",
        "                                break\n",
        "                    if 'end_time' in details:\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching auction details from {auction_url}: {e}\")\n",
        "\n",
        "        return details\n",
        "\n",
        "    def _find_auction_containers(self, soup):\n",
        "        \"\"\"Find auction containers on Earl's auction page\"\"\"\n",
        "        containers = []\n",
        "\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "        for link in all_links:\n",
        "            href = link['href']\n",
        "            if re.search(r'/auctions/\\d+', href) and 'lot' not in href:\n",
        "                parent = link.parent\n",
        "                if parent and len(parent.get_text().strip()) > 20:\n",
        "                    containers.append(parent)\n",
        "\n",
        "        if not containers:\n",
        "            selectors = [\n",
        "                'div[class*=\"auction\"]',\n",
        "                'div[class*=\"listing\"]',\n",
        "                'div[class*=\"card\"]',\n",
        "                'article',\n",
        "                '.auction-item',\n",
        "                '.listing-item'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors:\n",
        "                elements = soup.select(selector)\n",
        "                if elements:\n",
        "                    containers.extend(elements)\n",
        "\n",
        "        if not containers:\n",
        "            all_divs = soup.find_all('div')\n",
        "            for div in all_divs:\n",
        "                text = div.get_text().lower()\n",
        "                has_auction_content = any(keyword in text for keyword in ['auction', 'bidding', 'ends', 'lot'])\n",
        "                has_image = div.find('img') is not None\n",
        "                has_substantial_content = len(text.strip()) > 50\n",
        "\n",
        "                if has_auction_content and has_image and has_substantial_content:\n",
        "                    containers.append(div)\n",
        "\n",
        "        seen = set()\n",
        "        unique_containers = []\n",
        "        for container in containers:\n",
        "            container_id = id(container)\n",
        "            if container_id not in seen:\n",
        "                seen.add(container_id)\n",
        "                unique_containers.append(container)\n",
        "\n",
        "        return unique_containers[:20]\n",
        "\n",
        "    def _parse_auction_container(self, container):\n",
        "        \"\"\"Parse individual auction container from Earl's auction page\"\"\"\n",
        "        try:\n",
        "            title = self._extract_title(container)\n",
        "            if not title:\n",
        "                return None\n",
        "\n",
        "            auction_url = self._extract_auction_url(container)\n",
        "            auction_id = self.extract_auction_id(auction_url) if auction_url else None\n",
        "\n",
        "            location = self._extract_location(container)\n",
        "            dates = self._extract_dates(container)\n",
        "            end_time, time_remaining = self._extract_time_info(container)\n",
        "\n",
        "            auction = AuctionInfo(\n",
        "                company_name=\"Earl's Auction Company\",\n",
        "                company_url=auction_url or self.auctions_url,\n",
        "                auction_title=title,\n",
        "                dates=dates,\n",
        "                location=location,\n",
        "                bidding_notice=\"\",\n",
        "                zip_code=self.zip_code,\n",
        "                end_time=end_time,\n",
        "                time_remaining=time_remaining,\n",
        "                auction_id=auction_id\n",
        "            )\n",
        "\n",
        "            return auction\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing auction container: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_title(self, container):\n",
        "        \"\"\"Extract auction title from container\"\"\"\n",
        "        title_selectors = ['h1', 'h2', 'h3', 'h4', '.title', '.auction-title']\n",
        "\n",
        "        for selector in title_selectors:\n",
        "            element = container.select_one(selector)\n",
        "            if element:\n",
        "                text = element.get_text().strip()\n",
        "                if len(text) > 5:\n",
        "                    return text[:200]\n",
        "\n",
        "        texts = container.find_all(text=True)\n",
        "        for text in texts:\n",
        "            clean_text = text.strip()\n",
        "            if len(clean_text) > 10 and clean_text.upper() == clean_text and 'AUCTION' in clean_text:\n",
        "                return clean_text[:200]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_auction_url(self, container):\n",
        "        \"\"\"Extract auction URL from container\"\"\"\n",
        "        link = container.find('a', href=True)\n",
        "        if link:\n",
        "            href = link['href']\n",
        "            if '/auctions/' in href and '/lot/' not in href:\n",
        "                if href.startswith('http'):\n",
        "                    return href\n",
        "                else:\n",
        "                    return urljoin(self.base_url, href)\n",
        "\n",
        "        all_links = container.find_all('a', href=True)\n",
        "        for link in all_links:\n",
        "            href = link['href']\n",
        "            if re.search(r'/auctions/\\d+', href) and '/lot/' not in href:\n",
        "                if href.startswith('http'):\n",
        "                    return href\n",
        "                else:\n",
        "                    return urljoin(self.base_url, href)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_location(self, container):\n",
        "        \"\"\"Extract location from container\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        address_patterns = [\n",
        "            r'\\d+\\s+[A-Za-z\\s]+(?:Road|Street|Ave|Avenue|Blvd|Boulevard|Dr|Drive|Lane|Ln|Way|Court|Ct|Circle|Cir)',\n",
        "            r'[A-Za-z\\s]+,\\s*[A-Z]{2}\\s*\\d{5}'\n",
        "        ]\n",
        "\n",
        "        for pattern in address_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return match.group(0).strip()\n",
        "\n",
        "        city_state_match = re.search(r'([A-Za-z\\s]+),\\s*([A-Z]{2})', text)\n",
        "        if city_state_match:\n",
        "            return city_state_match.group(0)\n",
        "\n",
        "        return \"Indianapolis, IN\"\n",
        "\n",
        "    def _extract_dates(self, container):\n",
        "        \"\"\"Extract auction dates from container\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        date_patterns = [\n",
        "            r'\\d{1,2}/\\d{1,2}/\\d{4}',\n",
        "            r'[A-Za-z]+\\s+\\d{1,2},?\\s+\\d{4}',\n",
        "            r'\\d{1,2}-\\d{1,2}-\\d{4}'\n",
        "        ]\n",
        "\n",
        "        for pattern in date_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return match.group(0)\n",
        "\n",
        "        return \"TBD\"\n",
        "\n",
        "    def _extract_time_info(self, container):\n",
        "        \"\"\"Extract end time and time remaining from container\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        time_keywords = ['ends', 'ending', 'closes', 'closing', 'bidding', 'until']\n",
        "        for keyword in time_keywords:\n",
        "            if keyword in text.lower():\n",
        "                lines = text.split('\\n')\n",
        "                for line in lines:\n",
        "                    if keyword in line.lower():\n",
        "                        return self.parse_time_remaining(line)\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def scrape_auction_details(self, auction_url):\n",
        "        \"\"\"Scrape individual auction details and items\"\"\"\n",
        "        logger.info(f\"Scraping auction details: {auction_url}\")\n",
        "\n",
        "        content = self.get_page_content(auction_url)\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        items = []\n",
        "\n",
        "        auction_id = self.extract_auction_id(auction_url)\n",
        "        auction_title = self._extract_page_title(soup)\n",
        "\n",
        "        lot_links = self._find_lot_links(soup, auction_url)\n",
        "\n",
        "        if lot_links:\n",
        "            logger.info(f\"Found {len(lot_links)} lot links to scrape\")\n",
        "            for i, lot_url in enumerate(lot_links):\n",
        "                try:\n",
        "                    lot_item = self._scrape_individual_lot(lot_url, auction_id, auction_title)\n",
        "                    if lot_item:\n",
        "                        items.append(lot_item)\n",
        "\n",
        "                    if i < len(lot_links) - 1:\n",
        "                        time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error scraping lot {lot_url}: {e}\")\n",
        "                    continue\n",
        "        else:\n",
        "            logger.info(\"No individual lot links found, trying to parse auction page directly\")\n",
        "            item_containers = self._find_item_containers(soup)\n",
        "\n",
        "            for i, container in enumerate(item_containers):\n",
        "                try:\n",
        "                    item = self._parse_item_container(container, auction_id, auction_title, auction_url, i)\n",
        "                    if item:\n",
        "                        items.append(item)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error parsing item container {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        logger.info(f\"Found {len(items)} items in auction {auction_url}\")\n",
        "        return items\n",
        "\n",
        "    def _find_lot_links(self, soup, auction_url):\n",
        "        \"\"\"Find individual lot links on an auction page\"\"\"\n",
        "        lot_links = []\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "\n",
        "        for link in all_links:\n",
        "            href = link['href']\n",
        "            if '/lot/' in href:\n",
        "                full_url = urljoin(auction_url, href)\n",
        "                if re.search(r'/auctions/\\d+/lot/\\d+', full_url):\n",
        "                    lot_links.append(full_url)\n",
        "\n",
        "        seen = set()\n",
        "        unique_links = []\n",
        "        for link in lot_links:\n",
        "            if link not in seen:\n",
        "                seen.add(link)\n",
        "                unique_links.append(link)\n",
        "\n",
        "        return unique_links[:200]\n",
        "\n",
        "    def _scrape_individual_lot(self, lot_url, auction_id, auction_title):\n",
        "        \"\"\"Scrape an individual lot page with FIXED price parsing\"\"\"\n",
        "        logger.debug(f\"Scraping individual lot: {lot_url}\")\n",
        "\n",
        "        content = self.get_page_content(lot_url)\n",
        "        if not content:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            lot_match = re.search(r'/lot/(\\d+)', lot_url)\n",
        "            lot_number = lot_match.group(1) if lot_match else \"Unknown\"\n",
        "\n",
        "            description = self._extract_lot_description(soup, lot_url)\n",
        "            current_price, price_text = self._extract_lot_price_info(soup)\n",
        "            bid_count = self._extract_lot_bid_count(soup)\n",
        "            image_urls = self.extract_images(soup, lot_url)\n",
        "            end_time, time_remaining = self._extract_lot_time_info(soup)\n",
        "\n",
        "            item = self.create_auction_item_safely(\n",
        "                lot_number=lot_number,\n",
        "                description=description,\n",
        "                current_price=current_price,\n",
        "                price_text=price_text,\n",
        "                bid_count=bid_count,\n",
        "                source=lot_url,\n",
        "                auction_id=auction_id,\n",
        "                end_time=end_time,\n",
        "                time_remaining=time_remaining,\n",
        "                image_urls=image_urls,\n",
        "                auction_title=auction_title,\n",
        "                company_name=\"Earl's Auction Company\"\n",
        "            )\n",
        "\n",
        "            logger.debug(f\"Successfully parsed lot {lot_number}: {description[:50]}...\")\n",
        "            return item\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing individual lot {lot_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_lot_description(self, soup, lot_url):\n",
        "        \"\"\"Extract description from individual lot page\"\"\"\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            title_text = title_tag.get_text().strip()\n",
        "            title_text = re.sub(r\"\\s*-\\s*Earl's Auction Company.*$\", '', title_text)\n",
        "            if len(title_text) > 5:\n",
        "                return title_text[:300]\n",
        "\n",
        "        for heading_tag in ['h1', 'h2', 'h3']:\n",
        "            heading = soup.find(heading_tag)\n",
        "            if heading:\n",
        "                heading_text = heading.get_text().strip()\n",
        "                if len(heading_text) > 5 and \"Earl's Auction\" not in heading_text:\n",
        "                    return heading_text[:300]\n",
        "\n",
        "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "        if meta_desc and meta_desc.get('content'):\n",
        "            meta_text = meta_desc['content'].strip()\n",
        "            if len(meta_text) > 10:\n",
        "                return meta_text[:300]\n",
        "\n",
        "        url_match = re.search(r'/lot/\\d+-(.+)', lot_url)\n",
        "        if url_match:\n",
        "            url_desc = url_match.group(1).replace('-', ' ').title()\n",
        "            return url_desc[:300]\n",
        "\n",
        "        return \"Item description not available\"\n",
        "\n",
        "    def _extract_lot_price_info(self, soup):\n",
        "        \"\"\"Extract price information from individual lot page - FIXED VERSION\"\"\"\n",
        "        logger.debug(\"Starting price extraction from lot page\")\n",
        "\n",
        "        # Strategy 1: Look in specific elements that might contain price\n",
        "        price_selectors = [\n",
        "            '.current-bid', '.current-price', '.bid-amount', '.winning-bid', '.high-bid',\n",
        "            '[class*=\"bid\"]', '[class*=\"price\"]', '[id*=\"bid\"]', '[id*=\"price\"]'\n",
        "        ]\n",
        "\n",
        "        for selector in price_selectors:\n",
        "            elements = soup.select(selector)\n",
        "            for elem in elements:\n",
        "                text = elem.get_text().strip()\n",
        "                logger.debug(f\"Checking element {selector}: {repr(text)}\")\n",
        "                if text and len(text) < 50:  # Skip very long text\n",
        "                    price = self.parse_price(text)\n",
        "                    if price is not None:\n",
        "                        logger.info(f\"Found price ${price} in element {selector}\")\n",
        "                        return price, f\"${price:.2f}\"\n",
        "\n",
        "        # Strategy 2: Look for script tags with JSON price data\n",
        "        scripts = soup.find_all('script')\n",
        "        for script in scripts:\n",
        "            if script.string:\n",
        "                script_text = script.string\n",
        "                json_patterns = [\n",
        "                    r'\"current[_-]?bid\"[:\\s]*(\\d+(?:\\.\\d{2})?)',\n",
        "                    r'\"price\"[:\\s]*(\\d+(?:\\.\\d{2})?)',\n",
        "                    r'\"amount\"[:\\s]*(\\d+(?:\\.\\d{2})?)',\n",
        "                ]\n",
        "\n",
        "                for pattern in json_patterns:\n",
        "                    match = re.search(pattern, script_text, re.IGNORECASE)\n",
        "                    if match:\n",
        "                        try:\n",
        "                            price = float(match.group(1))\n",
        "                            if 0.01 <= price <= 50000:\n",
        "                                logger.info(f\"Found price ${price} in script tag\")\n",
        "                                return price, f\"${price:.2f}\"\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "        # Strategy 3: Look in page text line by line to avoid concatenation\n",
        "        page_text = soup.get_text()\n",
        "        lines = page_text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if len(line) < 100 and ('$' in line or 'bid' in line.lower() or 'price' in line.lower()):\n",
        "                logger.debug(f\"Checking line: {repr(line)}\")\n",
        "                price = self.parse_price(line)\n",
        "                if price is not None:\n",
        "                    logger.info(f\"Found price ${price} in page line\")\n",
        "                    return price, f\"${price:.2f}\"\n",
        "\n",
        "        # Strategy 4: Look for specific text patterns in small chunks\n",
        "        text_chunks = re.split(r'[.!?;]\\s+', page_text)\n",
        "        for chunk in text_chunks:\n",
        "            if len(chunk) < 200 and ('current bid' in chunk.lower() or 'winning bid' in chunk.lower()):\n",
        "                logger.debug(f\"Checking chunk: {repr(chunk[:100])}\")\n",
        "                price = self.parse_price(chunk)\n",
        "                if price is not None:\n",
        "                    logger.info(f\"Found price ${price} in text chunk\")\n",
        "                    return price, f\"${price:.2f}\"\n",
        "\n",
        "        logger.warning(\"Could not find any valid price information on page\")\n",
        "        return None, \"\"\n",
        "\n",
        "    def _extract_lot_bid_count(self, soup):\n",
        "        \"\"\"Extract bid count from individual lot page\"\"\"\n",
        "        bid_selectors = ['.bid-count', '.bids', '[class*=\"bid\"]']\n",
        "\n",
        "        for selector in bid_selectors:\n",
        "            bid_elem = soup.select_one(selector)\n",
        "            if bid_elem:\n",
        "                bid_text = bid_elem.get_text()\n",
        "                bid_match = re.search(r'(\\d+)', bid_text)\n",
        "                if bid_match:\n",
        "                    try:\n",
        "                        return int(bid_match.group(1))\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "        all_text = soup.get_text()\n",
        "        bid_patterns = [\n",
        "            r'(\\d+)\\s*Bids?',\n",
        "            r'Bids?[:\\s]*(\\d+)',\n",
        "            r'(\\d+)\\s*bidders?'\n",
        "        ]\n",
        "\n",
        "        for pattern in bid_patterns:\n",
        "            match = re.search(pattern, all_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    return int(match.group(1))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def _extract_lot_time_info(self, soup):\n",
        "        \"\"\"Extract time information from individual lot page\"\"\"\n",
        "        time_selectors = ['.end-time', '.closing-time', '.time-remaining', '[class*=\"time\"]']\n",
        "\n",
        "        for selector in time_selectors:\n",
        "            time_elem = soup.select_one(selector)\n",
        "            if time_elem:\n",
        "                time_text = time_elem.get_text()\n",
        "                end_time, time_remaining = self.parse_time_remaining(time_text)\n",
        "                if end_time:\n",
        "                    return end_time, time_remaining\n",
        "\n",
        "        all_text = soup.get_text()\n",
        "        time_keywords = ['ends', 'ending', 'closes', 'closing', 'time left', 'remaining']\n",
        "\n",
        "        for keyword in time_keywords:\n",
        "            if keyword in all_text.lower():\n",
        "                lines = all_text.split('\\n')\n",
        "                for line in lines:\n",
        "                    if keyword in line.lower():\n",
        "                        end_time, time_remaining = self.parse_time_remaining(line)\n",
        "                        if end_time:\n",
        "                            return end_time, time_remaining\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def _extract_page_title(self, soup):\n",
        "        \"\"\"Extract page title\"\"\"\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            return title_tag.get_text().strip()\n",
        "\n",
        "        h1 = soup.find('h1')\n",
        "        if h1:\n",
        "            return h1.get_text().strip()\n",
        "\n",
        "        return \"Earl's Auction\"\n",
        "\n",
        "    def _find_item_containers(self, soup):\n",
        "        \"\"\"Find item/lot containers on auction detail page\"\"\"\n",
        "        containers = []\n",
        "\n",
        "        selectors = [\n",
        "            'div[class*=\"lot\"]', 'div[class*=\"item\"]', 'tr[class*=\"lot\"]',\n",
        "            'tr[class*=\"item\"]', '.product', '.listing'\n",
        "        ]\n",
        "\n",
        "        for selector in selectors:\n",
        "            elements = soup.select(selector)\n",
        "            if elements:\n",
        "                containers.extend(elements)\n",
        "\n",
        "        if not containers:\n",
        "            rows = soup.find_all('tr')\n",
        "            for row in rows:\n",
        "                text = row.get_text().lower()\n",
        "                if any(keyword in text for keyword in ['lot', 'item', 'bid', '$']):\n",
        "                    containers.append(row)\n",
        "\n",
        "        return containers[:100]\n",
        "\n",
        "    def _parse_item_container(self, container, auction_id, auction_title, auction_url, index):\n",
        "        \"\"\"Parse individual item container with FIXED price parsing\"\"\"\n",
        "        try:\n",
        "            lot_number = self._extract_lot_number(container, index)\n",
        "            description = self._extract_description(container)\n",
        "            if not description or len(description) < 5:\n",
        "                return None\n",
        "\n",
        "            current_price, price_text = self._extract_price_info(container)\n",
        "            bid_count = self._extract_bid_count(container)\n",
        "            image_urls = self.extract_images(container, auction_url)\n",
        "            end_time, time_remaining = self._extract_item_time_info(container)\n",
        "\n",
        "            item = self.create_auction_item_safely(\n",
        "                lot_number=lot_number,\n",
        "                description=description,\n",
        "                current_price=current_price,\n",
        "                price_text=price_text,\n",
        "                bid_count=bid_count,\n",
        "                source=auction_url,\n",
        "                auction_id=auction_id,\n",
        "                end_time=end_time,\n",
        "                time_remaining=time_remaining,\n",
        "                image_urls=image_urls,\n",
        "                auction_title=auction_title,\n",
        "                company_name=\"Earl's Auction Company\"\n",
        "            )\n",
        "\n",
        "            return item\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing item: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_lot_number(self, container, index):\n",
        "        \"\"\"Extract lot number from container\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        patterns = [\n",
        "            r'(?:Lot|Item|#)\\s*[:\\-]?\\s*(\\d+[a-zA-Z]?)',\n",
        "            r'(?:^|\\s)(\\d+[a-zA-Z]?)[:\\-]',\n",
        "            r'#(\\d+[a-zA-Z]?)',\n",
        "            r'(\\d{1,4}[a-zA-Z]?)\\s*(?:\\.|:|\\-)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                lot_num = match.group(1)\n",
        "                if lot_num.isdigit() and 1 <= int(lot_num) <= 9999:\n",
        "                    return lot_num\n",
        "                elif len(lot_num) <= 6:\n",
        "                    return lot_num\n",
        "\n",
        "        return str(index + 1)\n",
        "\n",
        "    def _extract_description(self, container):\n",
        "        \"\"\"Extract item description\"\"\"\n",
        "        desc_selectors = ['.description', '.title', '.name', 'h1', 'h2', 'h3', 'h4', 'strong', 'b']\n",
        "\n",
        "        for selector in desc_selectors:\n",
        "            element = container.select_one(selector)\n",
        "            if element:\n",
        "                text = element.get_text().strip()\n",
        "                if len(text) > 5 and not re.match(r'^(Lot|#|\\d+)', text):\n",
        "                    return text[:300]\n",
        "\n",
        "        text_parts = [part.strip() for part in container.get_text().split('\\n') if part.strip()]\n",
        "        for part in text_parts:\n",
        "            if (len(part) > 15 and\n",
        "                not re.match(r'^(Lot|#|\\d+)', part) and\n",
        "                '$' not in part and\n",
        "                'bid' not in part.lower()):\n",
        "                return part[:300]\n",
        "\n",
        "        return \"Item description not available\"\n",
        "\n",
        "    def _extract_price_info(self, container):\n",
        "        \"\"\"Extract price information from container - FIXED VERSION\"\"\"\n",
        "        logger.debug(\"Extracting price from container\")\n",
        "\n",
        "        # Get text but limit the amount we process to prevent concatenation\n",
        "        text = container.get_text()\n",
        "        if len(text) > 500:\n",
        "            # If text is very long, try to find relevant sections\n",
        "            sentences = text.split('.')\n",
        "            relevant_text = \"\"\n",
        "            for sentence in sentences:\n",
        "                if any(keyword in sentence.lower() for keyword in ['bid', 'price', ']):\n",
        "                    relevant_text += sentence + \". \"\n",
        "            if relevant_text:\n",
        "                text = relevant_text\n",
        "            else:\n",
        "                text = text[:500]  # Just take first 500 chars as fallback\n",
        "\n",
        "        logger.debug(f\"Processing container text: {repr(text[:200])}\")\n",
        "\n",
        "        price = self.parse_price(text)\n",
        "        if price is not None:\n",
        "            return price, f\"${price:.2f}\"\n",
        "\n",
        "        return None, \"\"\n",
        "\n",
        "    def _extract_bid_count(self, container):\n",
        "        \"\"\"Extract bid count\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        bid_patterns = [\n",
        "            r'(\\d+)\\s*Bids?',\n",
        "            r'Bids?[:\\s]*(\\d+)',\n",
        "            r'(\\d+)\\s*(?:bidders?|bids?)'\n",
        "        ]\n",
        "\n",
        "        for pattern in bid_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    return int(match.group(1))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def _extract_item_time_info(self, container):\n",
        "        \"\"\"Extract time information for individual item\"\"\"\n",
        "        text = container.get_text()\n",
        "\n",
        "        time_elem = container.find(text=re.compile(r'End|Time.*Left|Closing', re.IGNORECASE))\n",
        "        if time_elem and time_elem.parent:\n",
        "            time_text = time_elem.parent.get_text()\n",
        "            return self.parse_time_remaining(time_text)\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def scrape_and_store_all(self, include_individual_auctions=True, max_auctions=5):\n",
        "        \"\"\"Main method to scrape and store all items in database\"\"\"\n",
        "        logger.info(\"Starting Earl's Auction scraper...\")\n",
        "\n",
        "        all_items = []\n",
        "        all_auctions = []\n",
        "\n",
        "        try:\n",
        "            auctions = self.scrape_earls_auctions()\n",
        "            all_auctions.extend(auctions)\n",
        "\n",
        "            if include_individual_auctions:\n",
        "                processed_auctions = 0\n",
        "                for auction in auctions:\n",
        "                    if processed_auctions >= max_auctions:\n",
        "                        break\n",
        "\n",
        "                    if auction.company_url:\n",
        "                        items = self.scrape_auction_details(auction.company_url)\n",
        "                        all_items.extend(items)\n",
        "                        processed_auctions += 1\n",
        "                        time.sleep(2)\n",
        "\n",
        "            stored_auctions = 0\n",
        "            stored_items = 0\n",
        "\n",
        "            for auction in all_auctions:\n",
        "                try:\n",
        "                    self.db.save_auction(auction)\n",
        "                    stored_auctions += 1\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error storing auction: {e}\")\n",
        "\n",
        "            for item in all_items:\n",
        "                try:\n",
        "                    self.db.save_item(item)\n",
        "                    stored_items += 1\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error storing item: {e}\")\n",
        "\n",
        "            logger.info(f\"Stored {stored_auctions} auctions and {stored_items} items in database\")\n",
        "            return all_items, all_auctions\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in scrape_and_store_all: {e}\")\n",
        "            return [], []\n",
        "\n",
        "    def scrape_zip_code_auctions_enhanced(self, max_auctions=10):\n",
        "        \"\"\"Enhanced zip code auction scraping\"\"\"\n",
        "        auctions = self.scrape_earls_auctions()\n",
        "\n",
        "        if self.zip_code:\n",
        "            for auction in auctions:\n",
        "                auction.zip_code = self.zip_code\n",
        "\n",
        "        return auctions[:max_auctions]\n",
        "\n",
        "    def scrape_individual_auctions_enhanced(self, max_auctions=5):\n",
        "        \"\"\"Enhanced individual auction scraping\"\"\"\n",
        "        auctions = self.scrape_earls_auctions()\n",
        "        all_items = []\n",
        "\n",
        "        processed = 0\n",
        "        for auction in auctions:\n",
        "            if processed >= max_auctions:\n",
        "                break\n",
        "\n",
        "            if auction.company_url:\n",
        "                items = self.scrape_auction_details(auction.company_url)\n",
        "                all_items.extend(items)\n",
        "                processed += 1\n",
        "                time.sleep(2)\n",
        "\n",
        "        return all_items\n",
        "\n",
        "    def scrape_catalog_page_enhanced(self, catalog_url):\n",
        "        \"\"\"Enhanced catalog page scraping\"\"\"\n",
        "        return self.scrape_auction_details(catalog_url)\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "def get_recent_auctions(db_path='hibid_auctions.db', zip_code=None, hours=24):\n",
        "    \"\"\"Get recent auctions from database\"\"\"\n",
        "    db = DatabaseManager(db_path)\n",
        "    return db.get_active_auctions(zip_code)\n",
        "\n",
        "def get_auction_items(auction_id, db_path='hibid_auctions.db'):\n",
        "    \"\"\"Get all items for a specific auction\"\"\"\n",
        "    db = DatabaseManager(db_path)\n",
        "    return db.get_items_by_auction(auction_id)\n",
        "\n",
        "def search_items(query, db_path='hibid_auctions.db', limit=50):\n",
        "    \"\"\"Search for items by description\"\"\"\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT i.*, GROUP_CONCAT(img.image_url) as image_urls\n",
        "            FROM items i\n",
        "            LEFT JOIN item_images img ON i.id = img.item_id\n",
        "            WHERE i.description LIKE ?\n",
        "            AND datetime(i.scraped_at) > datetime('now', '-24 hours')\n",
        "            GROUP BY i.id\n",
        "            ORDER BY i.current_price DESC\n",
        "            LIMIT ?\n",
        "        ''', (f'%{query}%', limit))\n",
        "\n",
        "        columns = [desc[0] for desc in cursor.description]\n",
        "        items = []\n",
        "        for row in cursor.fetchall():\n",
        "            item = dict(zip(columns, row))\n",
        "            if item['image_urls']:\n",
        "                item['image_urls'] = item['image_urls'].split(',')\n",
        "            else:\n",
        "                item['image_urls'] = []\n",
        "            items.append(item)\n",
        "        return items\n",
        "\n",
        "def scrape_earls_only(zip_code=None, max_auctions=5, db_path='hibid_auctions.db'):\n",
        "    \"\"\"Convenience function to scrape only Earl's auctions\"\"\"\n",
        "    scraper = EarlsAuctionScraper(zip_code=zip_code, db_path=db_path)\n",
        "    return scraper.scrape_and_store_all(\n",
        "        include_individual_auctions=True,\n",
        "        max_auctions=max_auctions\n",
        "    )\n",
        "\n",
        "def combine_hibid_and_earls(zip_code=None, max_auctions=3, db_path='hibid_auctions.db'):\n",
        "    \"\"\"Function to run both Hibid and Earl's scrapers\"\"\"\n",
        "    logger.info(\"Running combined Hibid + Earl's auction scraper...\")\n",
        "\n",
        "    all_items = []\n",
        "    all_auctions = []\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting Earl's Auction scraper...\")\n",
        "        earls_scraper = EarlsAuctionScraper(zip_code=zip_code, db_path=db_path)\n",
        "        earls_items, earls_auctions = earls_scraper.scrape_and_store_all(\n",
        "            include_individual_auctions=True,\n",
        "            max_auctions=max_auctions\n",
        "        )\n",
        "        all_items.extend(earls_items)\n",
        "        all_auctions.extend(earls_auctions)\n",
        "\n",
        "        logger.info(f\"Combined scraping complete: {len(all_items)} items, {len(all_auctions)} auctions\")\n",
        "        return all_items, all_auctions\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in combined scraping: {e}\")\n",
        "        return [], []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "eUR8RmBUMeLA",
        "outputId": "683aac45-9377-429f-f719-d5abfe14ff74"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1165) (<ipython-input-63-4dda38f39b73>, line 1165)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-4dda38f39b73>\"\u001b[0;36m, line \u001b[0;32m1165\u001b[0m\n\u001b[0;31m    ' not in part and\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1165)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_code = \"46074\"  # Change this to your desired zip code\n",
        "earls_scraper = EarlsAuctionScraper(zip_code=zip_code, db_path='hibid_auctions.db')\n",
        "earls_items, earls_auctions = earls_scraper.scrape_and_store_all(\n",
        "    include_individual_auctions=True,\n",
        "    max_auctions=3\n",
        ")"
      ],
      "metadata": {
        "id": "PIVCTKVzOuQb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(earls_items)"
      ],
      "metadata": {
        "id": "6f-lz3DPnhU6",
        "outputId": "b6958c9e-c174-46b6-c860-58dcfa561597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earls_items[6]"
      ],
      "metadata": {
        "id": "RpleSq9hm08T",
        "outputId": "776120f5-e1f8-4a54-fe12-3e0fcdc61bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuctionItem(lot_number='215246', description='MY 1ST WATER SLIDE SPLASH & SLIDE W/ TUNNEL IN BOX', current_price=6.4202530000652026e+25, price_text='$64202530000652025573933056.00', bid_count=6, source='https://www.earlsauction.com/auctions/10686/lot/215246-my-1st-water-slide-splash-and-slide-w-tunnel-in-box', auction_id='10686', end_time='2025-06-04T20:34:13.384419', time_remaining='3m', image_urls=['https://d3j17a2r8lnfte.cloudfront.net/eac/2025/6/medium/mWkIo5ga-AzPDLg8SsL2fKi2.jpeg', 'https://d3j17a2r8lnfte.cloudfront.net/eac/2025/6/medium/RlZE3rvjTGwCc2NeBWsBMdPJ.jpeg', 'https://d3j17a2r8lnfte.cloudfront.net/eac/2025/6/medium/5o02-o26eRotrjoXzjupuZBt.jpeg', 'https://d3j17a2r8lnfte.cloudfront.net/eac/2025/6/small/mWkIo5ga-AzPDLg8SsL2fKi2.jpeg', 'https://d3j17a2r8lnfte.cloudfront.net/eac/2025/6/small/RlZE3rvjTGwCc2NeBWsBMdPJ.jpeg'], auction_title=\"ENDS THURSDAY! ONLINE AUCTION 6/3-6/5 (BLUE) - Earl's Auction Company\", company_name=\"Earl's Auction Company\", scraped_at='2025-06-04T20:31:13.384452')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items[0]"
      ],
      "metadata": {
        "id": "NzJDm88ejq8c",
        "outputId": "2781d3fa-26e7-4354-e36d-d3f7eae04b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuctionItem(lot_number='5', description='Midwest Collectibles', current_price=None, price_text='', bid_count=0, source='https://hibid.com/indiana/auction/649047/walk-in-closet--sale', auction_id='649047', end_time=None, time_remaining=None, image_urls=['https://cdn.hibid.com/img.axd?id=8147704793&wid=&rwl=false&p=&ext=&w=0&h=0&t=&lp=&c=true&wt=false&sz=MAX&checksum=FwKT%2boo4SwiWT%2frBiKZgy7rnLKns32gX'], auction_title='WALK IN CLOSET  SALE\\n\\n\\n\\n\\n\\tOnline Only Auction', company_name='Midwest Collectibles', scraped_at='2025-06-04T20:07:57.448895')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}